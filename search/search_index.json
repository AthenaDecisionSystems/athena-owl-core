{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#owl-agent-framework-documentation","title":"Owl Agent Framework Documentation","text":"<ul> <li>Version 0.1: Created on 2024/03/01 - Updated on 2024/09/01</li> </ul>"},{"location":"#introduction","title":"Introduction","text":"<p>OwlAgent is an open source Hybrid AI framework for constructing interactive LLM-based assistants (e.g. chatbots) that include support for calling external decision services built with business rules via tool-calling to ensure precise decision-making at key moments by the interactive application. These applications can also access databases to get in-context information and a vector store of documents for doing RAG so that the chatbot can answer questions with relevant data and documents. Other tools can also be created for additional API access in the applications. This combination of technologies is called Hybrid AI and OwlAgent framework helps you create Agentic applications.</p>"},{"location":"#why-this-framework","title":"Why This Framework?","text":"<p>Today, most LLM-based applications in organizations focus on doing purely text-based tasks with a sprinkling of organizational knowledge from a set of documents - tasks like document or report summarization, answering questions whose answers can be found in documents, and \"conversing with data\" to get additional information about enterprise data.</p> <p>The next generation of applications will have to make precise, accurate, explainable decisions in context, leveraging the power of LLMs for natural language understanding and generation, but applying the organization's policies, contracts, and regulations to its data to help drive the business forward.   This can be used in places like customer service to answer specific customer queries about their individual situation, helping technicians maintain large equipment, and helping to automate complex tasks that currently require a lot of human time, for example translating complex purchase orders into bills of material for manufacturing.   These tasks require more than extracting information from documents - they require understanding natural language request intentions to make precise decisions by matching the right data to the right policies predictably and reliably, with clear explanations, and reformulating those decisions in natural language to make sense to end users.</p> <p>This class of applications requires a different approach, specifically for making decisions and getting the right data for those decisions.   It is not sufficient to simply ask LLMs, with their probabilistic models based on large correlations of language sequences, to reason about complex policy application decisions.   Their reasoning capabilities are not sufficient (indeed, one could argue they have none at all) and the need to get precise data to make these decisions requires additional reasoning and skill.   The best solutions will be hybrid - combining LLMs for natural language with additional tools for data management and decision-making - tools like business rules, knowledge graphs, and goal-oriented engines that know how to gather just the data needed to make decisions.</p> <p>The OwlAgent framework makes it as easy as possible to combine these different elements into a single interactive application, by minimizing the amount of custom development work so development teams can spend more time on developing decision services and thinking about UI and other issues.   It relies on common industry substrates like LangGraph and LangChain, and lets you use a variety of LLMs, vector stores, databases, decision engines, and other back-end tools needed for operations.</p> <p>We call this toolbox approach Hybrid AI.  You can read more about moving from a pre Generative AI approach to Hybrid AI here.</p>"},{"location":"#open-source-framework","title":"Open Source Framework","text":"<p>The OwlAgent framework is open source and we welcome all users and contributors!   Please write to Athena Decision Systems if you have any questions or need any support!   We are constantly iterating the framework and we hope you can use it successfully and ideally help out yourself!</p> <p>This documentation includes an overview of the approach, a getting started guide, some presentation material, links to demos and use cases.   Suggestions and corrections are welcome!</p>"},{"location":"#the-approach","title":"The Approach","text":"<p>The philosophy of the OwlAgent Framework is to be as declarative as possible, with almost all assistant components specified as parameters in a YAML file.   This greatly simplifies creating a new assistant, and allows assistants to run in a single server with a REST API.   In addition, there is a clean separation between the back end server and the front end, meaning you can write your own front end or use the assistants in a completely different type of application, simply by calling the REST APIs.</p> <p>The OwlAgent Framework was initially written by Athena Decision Systems but contributions are welcome from the whole community!   If you have any questions, please reach out by emailing Athena at contact@athenadecisions.com.</p> <p>The core OwlAgent framework leverages key parts of the generative AI and decision management ecosystem including:</p> <ul> <li>LangChain, LangGraph</li> <li>Any Large language model that supports tool calling, such as the GPT family, Claude 3.5, Mistral Large, Llama3, and some members of the Granite family, and others.</li> <li>Multiple model hosting environments (HuggingFace, IBM WatsonX.ai, Modal, AWS Bedrock) or local model hosting (e.g. using Ollama)</li> <li>Multiple Business Rule Management Systems for decision services (IBM ODM, Decision Management Open Edition, IBM Automated Decision Services, Open Source Kogito + potentially others)</li> </ul> <p>This video demonstration illustrates the value of this framework to help enterprises leverage Hybrid AI to help make even smarter interactive applications:</p> <p></p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>This repository includes the core elements of the OwlAgent Framework, while the demonstration git repository includes documentation on how to develop solutions from the core framework and leverage existing demonstrations.</p> <p>To get started we recommend cloning both repositories:</p> <pre><code>git clone https://github.com/AthenaDecisionSystems/athena-owl-core\ngit clone https://github.com/AthenaDecisionSystems/athena-owl-demos\n</code></pre> <p>And go to the demonstration documentation to run one of the existing demonstration or build your own solution.</p>"},{"location":"#owlassistants","title":"OwlAssistants","text":"<p>With the OwlAgent Framework, you can create assistants called OwlAssistants.   Each new use case or combination of components is an assistant.   For example, you can make an assistant for doing next best actions for customer service reps for your industry, or for helping maintenance technicians decide what inspections or maintenance operations to perform.   If you have a new decision service or domain model or functionality, you will typically make a new assistant.</p> <p>An OwlAssistant includes:</p> <ul> <li>a set of agents and tasks, defined in an assistant definition manifest (a YAML file).</li> <li>a set of tool definitions usable by LLM agents.</li> <li>an orchestration with LangGraph for stateful graph, or langchain as stateless chain.</li> </ul> <p>See architecture notes for details.</p> <p>Tools can be used to:</p> <ul> <li>access an integration layer to connect to custom data sources, business services, or decision services.</li> <li>execute rules to decide and run the next best action.</li> <li>read from a document storage reference and configuration to build corpus and vector store, </li> <li>do semantic search in a vector store with different collections,</li> <li>leverage third party API's to allow applications to take action in the world.</li> </ul> <p>The Owl Platform portal includes assistants, agents, and a tools hub that enterprises can use to let employees and other stakeholders select the best assistant for their tasks. The OwAssistant server is a deployable docker image that can run on any hybrid cloud platform.</p>"},{"location":"#contact-us","title":"Contact us","text":"<p>Athena Decision Systems is here to help you seamlessly integrate this innovative framework into your operations. For a customized proof of concept or production deployment, feel free to reach out to the expert team at Athena Decision Systems. With deep industry knowledge and tailored solutions, Athena can ensure a smooth and successful implementation that drives tangible value for your business.</p>"},{"location":"arch/","title":"Introduction","text":""},{"location":"arch/#architecture","title":"Architecture","text":""},{"location":"arch/#versions","title":"Versions","text":"<ul> <li>Created March 2024 - Updated 8/27/2024</li> </ul>"},{"location":"arch/#high-level-requirement-for-owlagent-framework","title":"High Level requirement for OwlAgent Framework","text":"<ul> <li> Ability to access information from corporate IT systems (typically databases or systems that host key corporate data such as CRM, Maximo, or ERP systems).</li> <li> Ability to leverage information found in corporate documents that help formulate responses or policy, on top of the formalized decisions made by decision services.</li> <li> Ability to access decision services by providing needed input parameters from a combination of chatbot context and enterprise IT data, and by injecting the decision service output back to the LLM\u2019s conversation context for output text generation.</li> <li> Ability to host the chatbot in multiple environments, including public cloud, private cloud, or enterprise data center.</li> <li> Easily create tools for decision services in multiple BRMS.</li> </ul>"},{"location":"arch/#system-context","title":"System Context","text":"<p>It is important to address the system context to define the OwlAgent's boundaries and its interactions with external entities (users, other systems, and any external interfaces). This diagram helps developers and architects gain a shared understanding of the project's scope and boundaries. It allows everyone involved to visualize the system's external entities and identify the various interactions that need to be supported.</p> <p></p> <p>The system context diagram aids in identifying the interfaces with external systems and services when the system is deployed.</p> <p>In this diagram we have the following external actors:</p> <ol> <li>End User interacting with the platform to get support to make decision using unstructured information in natural language. Classical user is a support team member try to get help, to better manage a customer reclamation, and get the next best action.</li> <li>ChatBot user interface: web application, at first deployed for internal staff, but with the capability to access on-premises backend and public APIs.</li> <li>LLMs API: most large language models will be accessed via public endpoint and API keys, but it supports on-premises LLM API with or without API key.</li> <li>Any business solution application needs to integrate with data-sources, and the goal is to expose those endpoint as tool calling capability so LLM can decide to make a call to those service. Those end points are REST and json based but could be SOAP web services too. Those business services include CRM, ERP, on-premises business applications, BPM workflow, etc.</li> <li>Structured data in SQL database may be directly used, and SQL queries may be integrated as function to be callable by tool calling.</li> <li>Unstructured data, in the form of key-value pair are persisted in Document Database and accessible via tool calling.</li> <li>An agent integrates with rule based systems - typically there will be a domain specific next-best action ruleset per deployed solution but the system integrates with existing decision services.</li> <li>A vector store to keep embeddings of domain-specific documentation and knowledge.</li> </ol>"},{"location":"arch/#component-view","title":"Component View","text":"<p>To zoom into the OwlAgent Framework backend server, we can highlight the following components, outside of any technology choices:</p> <p></p> <ol> <li>And end user interacts with a single page application (SPA) within a Web Browser. An admin user uses an SPA app with more panels.</li> <li>This SPA is served by a backend for front end or BFF, that can be deployed in different regions, worldwide or on-premises.</li> <li>OwlAgent exposes REST APIs to serve the SPA, with a different route for each different microservice.</li> <li>End-user management component, manages user group and user type to help controlling access to administration panels versus end user ones. It is a dedicated deployment unit. A microservice.</li> <li>Agent manager component creates and manages agents, which map to a use case specific to a business application or a business process that assists humans making decisions or taking actions. It persists metadata about the agent. An agent includes a prompt, a set of tools, tool mapping to Python functions, a vector store reference or collection within a vector store, a LLM API reference, and a LLM model reference. The agent is also a microservice, with CRUD operation on Agent. The code to support the integration of the agent is a dedicated class in the controller. Instantiation of this class is done via inversion of control and configuration.</li> <li>Prompt manager, manage the different prompts with its metadata. Prompt instances are linked to the agent and use case to implement. THey may be localized.</li> <li>Locale manager manages localization for English, French, Spanish and potentially other languages for user interface text.</li> <li>Conversation Manager manages conversations between the end user and the back-end server (mediated via a front-end server).  This component has a base implementation to serve Q&amp;A interaction and chat based conversation. It may be unique per deployed solution as it defines tool and agent-specific configuration.</li> <li>Tool manager manages tool definitions and code references that are instantiated during agent creation and use.</li> <li>Document management exposes document upload management, cloud storage access, metadata management, and trigger embeddings.  Document processing can be adapted for each solution deployment. This component should map the documents to domain vector store mapping.</li> <li>Vector Store manager: For good RAG, we need to have dedicated, domain specific vector stores or collections within a vector store. This component is responsible for the management of the metadata of the vector stores.</li> <li>Decision Service Connector: is a component that will do data mapping so that an existing decision service to be integrated as a tool in an agent.</li> <li>LLM connector is an integration layer to the different LLMs. It can include existing LangChain constructs, or special API one, or locally deployed LLM. LLM connectors to support are WatsonX.ai, OpenAI, Anthropic, Mixtral, Llama3, and any open source models remotely accessible.</li> </ol> <p>The platform supports services to address governance end-to-end, with security access control, logging and auditing. Conversation flows will be persisted for playbacks and lineage. Governance is a day one design decision for this platform.</p>"},{"location":"arch/#component-design","title":"Component design","text":"<p>The core server needs to support:</p> <ul> <li> The runtime as a web server serving OpenAPI REST apis. \u2192 FastAPI is used with the main function using middleware and routers athena.main.</li> <li> Docker image packaging for easy deployment on any platform. \u2192 Dockerfile and how to use it in the context of a demonstration is illustrated in the Docker-compose file</li> </ul> <pre><code>owl-backend:\n    hostname: owl-backend\n    image: athena/owl-backend\n    container_name: owl-backend\n    ports:\n    - 8000:8000\n    environment:\n    CONFIG_FILE: /app/config/config.yaml\n    OPENAI_API_KEY: $OPENAI_API_KEY\n    volumes:\n    - ../../ibu_backend/config:/app/config\n    - ../../.env:/app/.env\n    - ./data/file_content:/app/file_content\n    - ./data/chromadb:/app/chromadb\n    - ../../ibu_backend/src/ibu:/app/ibu\n</code></pre> <p>Configuration and tool code is mounted inside the docker container. The configuration defines the classes to instantiate in the different components that are linked to a specific agent.</p> <ul> <li> Default server configurations for a specific agent are defined in an external config.yaml</li> <li> A server can have different agent runners running in parallel in the context of user conversations.</li> <li> Server supports multiple end user conversations.</li> </ul>"},{"location":"arch/#architecture-approach","title":"Architecture Approach","text":"<p>When the backend server process starts, it loads the server configuration and keeps the configuration as a singleton. The server offers a set of health APIs and conversation APIs.</p> <p></p> <p>Each router api defines the HTTP verb and resource path and then delegates to another service implementation. Those services are singletons.</p>"},{"location":"arch/#conversation-orchestration","title":"Conversation orchestration","text":"<p>The conversation is what links the agent to a user. The user starts a conversation using a chatbot graphical user interface (aka a front-end server). The context of the conversation should include the <code>agent_id</code>, the <code>user_id</code>, and a <code>thread_id</code> to keep track of the conversation. As a conversation has history the <code>chat_history</code> is also kept.</p>"},{"location":"arch/#requirements","title":"Requirements","text":"<ul> <li> The conversation supports synchronous chat API</li> <li> The conversation supports asynchronous/ streaming chat API</li> <li> Need to identify a unique user to keep conversation states, and history</li> <li> Conversation states are persisted in remote database</li> <li> Conversation uses the agent_id to get an instance of the agent executor, to pass the conversation context.</li> </ul>"},{"location":"arch/#approach","title":"Approach","text":"<p>The conversation REST resource expose synchronous or async API and delegates to a conversation manager.</p> <p></p> <p>The conversation parameter includes the <code>user_id</code>, so the server manages multiple users in parallel, the <code>agent_id</code> as a conversation is linked to a use case thus to an agent. To get the agent executor the agent manager creates one instance according to the <code>OwlAgent</code> definition.</p> <p></p> <p>The agent runner instance exposes <code>invoke</code> or <code>stream</code> methods to send queries to the underlying LLM and provide the response back either as a single block or asynchronously via streaming.</p>"},{"location":"arch/#agent-manager","title":"Agent Manager","text":"<p>A specific business use case implementation is supported by one agent. An agent groups an LLM reference, a system prompt, and tool declarations. The agent manager manages this metadata in the OwlAgent object. The application uses an agent executor instance.  An agent may use multiple LLM/prompt so can be implemented with LangGraph. LangGraph brings the persistence of the conversation with the <code>thread_id</code> and the ability to play back the conversation, therefore it will be the preferred implementation choice.</p>"},{"location":"arch/#agent-requirements","title":"Agent Requirements","text":"<ul> <li> Define agents via a config file in yaml format, loaded and cached in the agent manager.</li> <li> An agent is defined by a unique id, a name, a description, some metadata, a class to support the implementation, and a list of tools.</li> <li> Agent manager manages CRUD operations of the AgentEntity.</li> <li> AgentEntity references the prompt to use and the model parameters like model name, temperature, top K, top P, etc.</li> </ul> <p>&gt;&gt;&gt; See detail design section</p>"},{"location":"arch/#document-manager","title":"Document Manager","text":""},{"location":"arch/#document-manager-requirements","title":"Document Manager Requirements","text":"<ul> <li> Support uploading different file formats including txt, markdown, PDF, MS Word, HTML, and others. The file may be in a remote repository accessible using a URL and so does not need to be moved to another location.</li> <li> Support keeping metadata about the file like type, ownership, catagories, tags.</li> <li> Support a file description dedicated repository layer, injectable, and configurable.</li> <li> Support splitting the document into smaller chunks.</li> <li> Support chunk embeddings and persistence into vector database (Support Chromadb).</li> <li> Support similarity search in indexes.</li> <li> (optional) Support persisting to a cloud object storage like S3. The bucket is owned by Athena, but prefix is dependent on the solution. [HED]: Bucket should be owned by end-user.</li> <li> Standalone deployment of this component as a Docker container deployable on any cloud or on-premises environment.</li> <li> Embeddings will depend on the LLM selected so need to support external configuration and IoC component.</li> </ul>"},{"location":"arch/#document-manager-approach","title":"Document Manager Approach","text":"<p>Expose a REST resource for document management with core processing dependent od configuration:</p> <p></p> <ul> <li>The file descriptor specifies some metadata about the file uploaded. The file can be uploaded using the REST operation or given as a URL to get the document from this URL.</li> <li>Configuration defines:</li> <li>name of the embeddings to use</li> <li>vector store used and path or URL when using remote access</li> </ul>"},{"location":"arch/#tool-manager","title":"Tool Manager","text":""},{"location":"arch/#prompt-manager","title":"Prompt manager","text":""},{"location":"arch/#prompt-manager-requirements","title":"Prompt Manager Requirements","text":"<ul> <li> Support prompt manager as a component, as a singleton, with prompts coming from one file.</li> <li> Cache the prompts in memory for demonstration purpose.</li> <li> Support adding new prompt with a unique key. If key exists then it is an update.</li> <li> Support get by key for a prompt.</li> <li> Support deleting a prompt given its key.</li> </ul>"},{"location":"arch/#prompt-manager-approach","title":"Prompt Manager Approach","text":"<p>For the open source version the persistence is in the file system. Default key name and file path to persist the prompts are in a config file. The <code>prompts.json</code> is created manually for the default prompt.</p>"},{"location":"arch/#relationship-between-components","title":"Relationship between components","text":"<p>An Agent includes a prompt, an LLM backend, a model reference, and tool references.</p> <pre><code>    ibu_insurance_agent:\n        agent_type: openai\n        model_ref:  gpt-3.5-turbo-0125\n        ow_agent_class: athena.llm.agent_openai.OpenAIClient\n        prompts:\n</code></pre>"},{"location":"arch/#physical-deployment","title":"Physical Deployment","text":""},{"location":"brms/","title":"Business Rule Management Systems","text":""},{"location":"design/","title":"OwlAgent design","text":""},{"location":"design/#owl-agent-backend-design","title":"Owl Agent Backend Design","text":"<p>This chapter explains the design, code, and implementation approach, of the OwlAgent Framework backend and how to continue developing and testing it.</p> <p>The framework's code is in the GitHub repository athena-owl-core/owl-agent-backend.</p>"},{"location":"design/#the-core-concepts","title":"The core concepts","text":"<p>The core concepts the framework manages are agents, tools, and prompts.   An agent is a deployable application built up by choreographing one or more llm, each with its own workflow that can leverage external tools, guided by prompts.</p> <p>Here is a how they are related:</p> <p></p> <p>An agent is an interactive application or solution that supports a specific business use case, like helping a worker performing a specific task of a business process.  The execution of the agent involves the coordination of one or more LLMs.  Agents may be stateful to preserve the state of a conversation using snapshot capabilities.</p> <p>An Agent manages a co-ordinated set of calls to a Large Language Model,  with a prompt and tools, to accomplish a subtask. A retriever is a tool that can access a document or collection of documents in a vector store. So implementing RAG in the OwlAgent Framework means using a retriever tool inside an agent.</p>"},{"location":"design/#code-organization","title":"Code organization","text":"<p>To find the code for the back-end, first clone the OwlAgent Framework core repository:</p> <pre><code>git clone AthenaDecisionSystems/owl-agent-core\n</code></pre> <p>Then look in the <code>athena-owl-core/owl-agent-backend</code> directory:</p> <pre><code>cd athena-owl-core/owl-agent-backend\n</code></pre> <p>The code for the backend is in the <code>src</code> folder, while unit tests and integration tests are under <code>tests/ut/</code> and <code>tests/it</code> respectivily.</p> <p>The backend runs as a server that can support multiple agents simultaneously. The backend is typically run in a Docker container and then accessed as a set of REST APIs. It is implemented using FastAPI. We recommend using Python 3.12 for development.</p> <p>The main entry point for the owl-backend is found in the Python file athena.main.py which is the core of the FastAPI server.   Other files implement different endpoint APIs that the backend server exposes; each one is considered a different component implementing a different set of related features of the backend.</p> <p>The backend can run in the uvicorn or ugnicorn server (the defaults for FastAPI). It exposes two set of APIs:</p> <ul> <li><code>/api/v1/c/</code> for managing conversations with the user, typically exposed in a chatbot user interface.  (The OwlAgent Framework frontend server is the default user interface but others can be developed by calling these API's directly.)</li> <li><code>/api/v1/a/</code> for administration tasks, such as managing the different OWL entities of the frameworks: agents, prompts, or tools.</li> </ul> <p>The <code>src</code> folder includes the Dockerfile to build the image, the <code>requirements.txt</code> for specifying Python module dependencies and a <code>start_backend.sh</code> script to enable local development tests. Unit and integration test are done using <code>pytest</code> and unittest modules. Code testing can be debugged in the VSCode IDE.</p> <p>Before starting work on the backend, it's important to set up your own Python virtual environment.   Make sure you have Python 3.12 installed and available as the command <code>python</code>, as well as <code>pip</code>.</p> <pre><code>python -m venv venv # you can choose your own name of course\nsource venv/bin/activate # on Windows type 'venv\\Scripts\\activate'\ncd athena-owl-core/owl-agent-backend/src\npip install -r requirements.txt\n</code></pre>"},{"location":"design/#important-components","title":"Important components","text":"<p>The architecture document presents the components. Each component has its APIs implemented with service /repository code, and then LLM-specific facade code as needed.</p>"},{"location":"design/#conversation","title":"Conversation","text":"<p>The conversation API is in routers/conversations.py.</p> <p>An OwlAgent conversation is very similar to a thread in the OpenAI Assistant API.</p> <p></p> <p>The conversation manager conversation_mgr.py in the <code>llm/conversations</code> folder exposes a factory method to create, or get from the cache, the agent supporting the conversation.</p> <p>When an agent is not in memory, the factory delegates the creation of the agent to the agent manager.  (It sounds like middle management work, doesn't it?)</p> <p><code>Conversation</code> uses a <code>ConversationControl</code> bean class to describe its state and parameters. The definitions of these classes are in a DTO (Data Transfer Object) model so it can be easily shared with other apps.</p> <p>If in the user interface, the end user selects another agent for their interactions, a new conversation should be started. When an agent has a stateful implementation, such as LangGraph, then the conversation will be saved as part of the memory management of LangGraph using the thread's unique identifier.</p> <p>Here is an example of a simple query to the Anthropic Claude LLM using an agent that has Tavily search tool. The payload to the POST url is</p> <pre><code>{\n\n  \"query\": \"What does the Athena Decision Systems company do?\",\n  \"user_id\": \"joel\",\n  \"agent_id\": \"base_tool_graph_agent\",\n  \"thread_id\": \"1\"\n}\n</code></pre> <p>From a conversation interaction, the sequence flow looks like in the following sequence diagram:</p> <p></p> <p>The Agent Runners are instances created by factory classes using the parameters read in the appropriate agent definition.  These definitions are stored in yaml files as described below.</p>"},{"location":"design/#agents","title":"Agents","text":"<p>Agent management has two parts: 1/ the management of the OwlAgent entity definitions with REST resources and a persistence repository, and 2/ the agent runner instance which manages conversations:</p> <p></p>"},{"location":"design/#agent-entity-management","title":"Agent Entity Management","text":"<p>The agent REST resource defines the FastAPI router (see code in <code>routers/agents.py</code>) and the CRUD verbs.</p> <p></p> <p>The REST resource and APIs are defined in agents.py.</p> <p>The following code illustrates simple delegation to the agent manager:</p> <pre><code>from athena.llm.agents.agent_mgr import get_agent_manager, OwlAgent\n\nrouter = APIRouter( prefix= get_config().api_route +\"/a\")\n\n@router.get(\"/agents/{id}\")\ndef get_agent_entity_by_id(id: str) -&gt; OwlAgentEntity:\n    return get_agent_manager().get_agent_by_id(id)\n</code></pre> <p>The Agent manager is in llm/agents folder. The current implementation uses a local file to keep OwlAgent definitions . The agent id needs to be unique among the declaration of all agents.</p> <pre><code>class OwlAgent(BaseModel):\n    agent_id: str \n    name: str \n    description: Optional[str]\n    modelName: str \n    modelClassName: Optional[str] \n    class_name: str \n    prompt_ref:  str \n    temperature: int = 0 \n    top_k: int = 1\n    top_p: int = 1\n    tools: list[str] = []\n</code></pre> <p>The name is purely for user interface display as is the description. The description should clearly state the intent of the agent for a human end user. An agent includes one system prompt (see the prompt design below). As of now the <code>modelClassName</code> must name a class from the LangChain chat API. The <code>modelName</code> is the specific LLM name to use. An agent can have 0 to many tools.</p> <p>Here is an example of an OwlAgent definition:</p> <pre><code>openai_tool_chain:\n  agent_id: openai_tool_chain\n  name: open_ai_gpt35\n  description: openai based agent with prompt coming from langchain hub  and tool\n  runner_class_name: athena.llm.agents.agent_mgr.OwlAgentDefaultRunner\n  modelName: gpt-3.5-turbo\n  modelClassName: langchain_openai.ChatOpenAI\n  prompt_ref: openai_functions_prompt\n  temperature: 0\n  top_k: 1\n  top_p: 1\n  tools:\n  - tavily\n</code></pre>"},{"location":"design/#agent-runner","title":"Agent Runner","text":"<p>The agent manager exposes a factory method to create agent executor using the OwlAgent entity information. The LLM class is also define in the descriptor.</p> <p>The validation unit tests are in tests/ut/test_agent_mg.py and the integration tests  in tests/it/test_agents_api.py</p> <p>The <code>llm/agents</code> folder includes some pre-defined agents (new ones being added regularly):</p> Agent Description fake_agent To do unit testing without cost OwlAgentDefaultRunner The default agent runnung using Langchain chain, and dynamic instantiation of the LLM class base_graph_agent A default agent to do simple LLM calls including tool calling. It uses one agent with a LangChain chain. LLM is pluggable Base Tool Graph Agent A LangGraph flow with an agent and tool nodes, like the ReAct pattern. <p>The configuration of the agents are in the config/agents.yaml files</p> <p>You can approach new agent development in a few ways:</p> <ol> <li>Use existing agent code and define new tools and prompts.</li> <li>Use existing agents and combine them in a LangGraph graph with tools and prompts using a new agent implementation.</li> <li>Develop a new agent code for a new LLM, with a new prompt and tools.</li> </ol>"},{"location":"design/#tools","title":"Tools","text":"<p>The concept of calling external tools, also called functions, was introduced by OpenAI, and most major LLMs now support it.</p> <p>Here is an example of python function that can be used as a tool:</p> <pre><code>def query_crm_backend(query: str):\n    \"\"\"Call the customer relationship management (CRM) to get customer data.\"\"\"\n\n    return [\"The customer records from DEMO CRM\"]\n</code></pre> <p>An example of this function as a tool is in demo_tools.py</p> <p>The OWL Framework uses a tool factory to create tool references used by LLM API.</p>"},{"location":"design/#langchain-tool-api","title":"Langchain tool API","text":"<p>The simple way to add tools to an LLM instance in LangChain is to define an AgentExecutor, which combines an LLM and a prompt with the tool names.</p> <pre><code>agent = LLMSingleActionAgent(llm_chain=llm_chain, output_parser=output_parser,\n    stop=[\"\\nObservation:\"], allowed_tools=tool_names,\n)\nagent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools)\n</code></pre> <p><code>tool_names</code> is a list of strings of the names of the tools, while <code>tools</code> is a list of python functions implementing the names.</p> <p>Zooming to the prompt, it needs to include placeholders for tools and tool_names:</p> <pre><code>**Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:**\n\n**{tools}**\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of **[{tool_names}]**\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\nQuestion: **{input}** \\n    **{agent_scratchpad}**\n</code></pre> <p>The Python module that manages the prompt needs to implement the tool factory class:</p> <pre><code>from athena.llm.tools.tool_factory import ToolInstanceFactoryInterface\nclass DemoToolInstanceFactory(ToolInstanceFactoryInterface):\n  def build_tool_instances(self, tool_entities: list[OwlToolEntity]) -&gt; list[Any]:\n</code></pre> <p>This factory uses LangChain to build StructuredTool:</p> <pre><code>def build_tool_instances(self, tool_entities: list[OwlToolEntity]) -&gt; list[Any]:\n    \"\"\" From the list of tools to use build the function reference for LLM \"\"\"\n    tool_list=[]\n    for tool_entity in tool_entities:\n        tool_list.append(self.define_tool(tool_entity.tool_description, tool_entity.tool_fct_name, tool_entity.tool_arg_schema_class))\n    return tool_list\n</code></pre> <p>The <code>tools.yaml</code> file includes the definition of the tool as a OwlEntity:</p> <pre><code>query_crm:\n  tool_id: query_crm\n  tool_class_name: athena.llm.tools.demo_tools\n  tool_description: \"\"\"Call the customer relationship management (CRM) to get customer data.\"\"\"\n  tool_fct_name: 'query_crm_backend' \n</code></pre> <p>When creating the agent, the tool definitions are loaded and then passed to the <code>build_tool_instances()</code> function. See the code</p> <p>[HED] How do the tool descriptions and the tool parameters and their descriptions get used?</p>"},{"location":"design/#prompts","title":"Prompts","text":"<p>TBD</p>"},{"location":"design/#document-management","title":"Document management","text":"<p>When a user uploads a document using the chatbot interface, the file is persisted in a cloud object storage with some metadata. The file is parsed into sub-documents or chunks that are vectorized via <code>Embeddings</code>. The created vectors are saved in a vector store. The architecture section showed an overview of vector store processing.</p> <p>The REST resource is in the document.py file. It has two APIs, one for similarity search and one to upload a document. In this snippet, <code>file_description</code> represents the file's metadata and <code>myFile</code> the binary stream coming from the client application:</p> <pre><code>@router.post(\"/documents\")\nasync def post_document_for_rag( file_description: FileDescription = Depends(), myFile: UploadFile = File(...)):\n    # delegate to the document manager\n</code></pre> <p>The document manager is in the itg/store/content_mgr.py file. The logic to process the uploaded file is:</p> <ol> <li>Persist the metadata file and potentially the file content itself in the storage URI as specified in the <code>config.yaml</code> file: <code>owl_agent_content_file_path</code>.</li> <li>From the document type, perform chunking and embedding separately, as the libraries to parse and split the main document are different.</li> <li>Create embeddings and save them in the vector store in the collection as defined by the config file.</li> </ol> <p>The content_mgr offers a <code>get_retriever()</code> method to be using in LLM RAG implementation.</p>"},{"location":"gen_ai_gs/","title":"Gen AI to Hybrid AI","text":""},{"location":"gen_ai_gs/#from-generative-ai-to-hybrid-ai","title":"From Generative AI to Hybrid AI","text":""},{"location":"gen_ai_gs/#introduction","title":"Introduction","text":"<p>Generative AI uses special types of neural network models to generate new content (text, images, music, videos, and so on) based on a request that is typically text. Models are pre-trained on vast amounts of unlabeled data, using from 7B to 500B parameters. The models are trained on terabytes of data including books, articles, websites, and whatever else the model trainers can find and feel they can use. This helps the model simulate learning grammar, facts, reasoning abilities and even some level of common sense from the content - because there is a rich store of sequential patterns in the training data to draw on.</p> <p>Gen AI's natural language understanding and generation capabilities applies well to different use cases: improving customer experience through natural language, improving an employee's productivity, provoking creativity by generating ideas based on identifying deep correlations in seemingly unrelated areas, and optimizing certain business processes. Overall, generative AI sometimes seems like magic - and at first it seems like Generative AI is a general intelligence because we humans always associate high level of natural language ability with high intelligence.</p>"},{"location":"gen_ai_gs/#key-concepts","title":"Key Concepts","text":"<p>Generative AI today is often used via a chatbot hosted by an AI vendor like OpenAI, Anthropic, IBM, or Mistral. Most of the platforms, including IBM's <code>watsonx.ai</code>, use Large Language Models (LLM) with many query types. The architecture of such a system looks like this:</p> <p></p> <p>LLMs are typically hosted on dedicated hardware with GPUs that are needed for high-speed inference.  (Inference is the technical term for generating new content with a model.)  LLMs are primarily used to do natural language processing (NLP) of unstructured queries from human users. The interactions are stateless, and the concept of a context window is used to include contextual data as part of the query.   Contextual data includes previous dialog in a conversation, system prompts, relevant bits of information from other documents (called RAG), or data brought in from databases or external API service calls.  As the LLM is essentially stateless, any information used to generate a response must be included in the context, which is why context window size is very important.   Since most AI companies charge per token consumed and generated, the more context that is included in each LLM query, the more expensive the query.<sup>1</sup></p> <p></p> <p>IBM's watsonx.ai, for example, helps users select which LLM to use, define system prompts, and perform queries. As an enhanced platform, AI scientists can also fine-tune an existing open-source model, or develop brand new ML models.</p> <p>The same approach exists for vendors like Anthropic, OpenAI, Perplexity, Mistral, Hugging Face, Anakin AI, AWS Bedrock, and so on.</p> <p>Models used for inference are accessible via an API.</p> <p>Enterprise deployments of Generative AI need a high degree integration, since enterprise applications need to leverage enterprise-specific instructions, documents, services, and data to perform useful functions for the company. Some tools are already available to help develop Generative AI workflows: For instance watsonx Orchestrate has a no-code approach for leveraging a library of skills. For developers, lower level frameworks, such as LangChain, LlamaIndex, and Haystack offer Python-based libraries with enormous flexibility and a high degree of complexity.</p> <p>A typical real deployment looks like the following figure:</p> <p></p> <p>The backend for a chatbot application needs to be integrated with existing data sources and services including databases, SOA services, Customer Relationship Management platforms, predictive machine learning models and scoring engines, business process management workflows -- and of course business-rule or optimization model based decision services... The data from these services need to be included in a conversation with a human user if it is to be relevant to the user's current context.   Fine-tuning the model with the latest information is infeasible when enterprise data changes every second and when decision models can change on a daily basis, so a dynamic approach to getting the data into the LLM's context window is needed.   In addition, in order to effect changes in the enterprise environment, the user needs to be able to update data or call services.</p> <p>There are also more and more models that can be hosted for inference on smaller devices with less specialized hardware, which may have significant inference cost and performance improvements vs cloud-hosted models.  But hosting these models in an enterprise still requires creating and managing the appropriate environments and handling governance as models evolve as well as secure access to the models.</p> <p>Enterprises have a heterogenous IT environment; solutions need to be deployed on virtual private clouds, on-premises servers, or in an hybrid cloud depending on the enterprise's policies and IT landscape.  Other IT considerations such as security and authentication are also paramount.</p> <p>All of this means that when an enterprise wants to deploy an LLM-based solution in production, there will be significant integration and development effort.   The OwlAgent Framework attempts to minimize the development effort by providing simple abstractions and a declarative way of combining different components in a specific application.</p>"},{"location":"gen_ai_gs/#challenges","title":"Challenges","text":"<p>LLM's are amazing tools for doing natural language processing.   But they come with challenges due to the underlying training and inference technology, due to the fact they are trained only occasionally and are thus the information they use is always out of date, and due to the fact that natural language generation is not grounded in any model of reality or reasoning but instead uses probabilistic techniques based on correlations of a huge number of strings of tokens (words). This means that hallucination and approximate retrieval are core to their architecture: the completions they generate have the same statistical distribution as the text they have been trained on. Prompt engineering cannot eliminate hallucination since the decision to assess the response as a factual completion depends on the knowledge of the prompter and requires continuously assessing all the responses.   Various techniques to reduce the impact of this lack of groundedness, such as using multiple LLMs to judge each others' answers, are expensive and only move the problem around.</p> <ul> <li> <p>Accuracy: The accuracy of LLM's in answering precise questions about enterprise decisions is not acceptable to any enterprise that must follow regulations and policies and respect contractual agreements with suppliers and customers.   Because LLMs cannot truly reason or take into account regulations and policies precisely, models often produce incorrect and contradictory answers when asked for decisions or actions to undertake.  With classical ML, probabilistic output is expected and scores are combined with thresholds and rules to make final decisions. Symbolic approaches like business rules that precisely express policies produce reliable results at the cost of coding the policies mostly manually.</p> </li> <li> <p>Specificity: A single large model is unlikely to solve every business problem effectively because it is trained on generally-available information rather than enterprise-specific information. To differentiate their generative AI applications and achieve optimal performance, companies should rely on their own data sets tailored to their unique use case.   Even then, enterprise data changes constantly, so techniques such as RAG and tool calling are needed to leverage the most up-to-date and relevant information for a specific query.</p> </li> <li> <p>Cost and Risk of training and inference, as well as privacy and intellectual property are key concerns. LLM's can be \"fine-tuned\" for a specific task by using a small number of labeled examples specific to the company's industry or use case. Fine-tuned models can deliver more accurate and relevant outputs. But training and retraining models, hosting them, and doing inference with them are expensive. Cloud providers see this opportunity to sell more virtual servers equipped with GPU's at a higher price.  In addition, the fine-tuning needs to be redone and retested whenever the information used for it changes, or whenever the underlying model is updated.   Therefore, fine-tuning has only limited appeal in practice.</p> </li> <li> <p>Skills: Developing a new LLM for an enterprise usually makes little sense today, but fine tuning an existing model may in some circumstances. There are relatively few developers with expertise in model tuning, understanding their architecture and limitations, integrating them in applications, and in tuning their hyper parameters. Reinforcement learning to fine-tune existing LLM requires a huge number of trials, and data quality is still a very difficult and poorly-mastered topic.</p> </li> <li> <p>Reliability and reasoning: Generative AI models do not reason and do not plan accurately and consistently, despite multiple prompting techniques designed to get them to do so. New versions of LLMs attempt to improve this, but by design the transformer algorithm is probabilistic and greedy for text generation and does not inherently do any kind of structured symbolic reasoning or manage ontologies of concepts (knowledge graphs). LLM are very big system-1 with their knowledge based from digital representation of humanity created content.  In addition, even if they are able to make simple plans and reason over simple cases, enterprise decisions require very specific, detailed, and complex context and instructions, well beyond the capacity of any LLM reasoning techniques.  Imagine, for example, relying on chain-of-thought reasoning to get an LLM to provide expert assistance to an airplane maintenance technician on how to repair a jet engine...</p> </li> </ul> <p>For all of these reasons, we remain firmly convinced that the right way to build AI-based enterprise solutions today is by using a combination of technologies, where generative AI and LLMs play a key role, but other technologies such as symbolic rule-based decision engines are equally key.</p> <p>The next sections explains the generative AI architecture in more detail.</p>"},{"location":"gen_ai_gs/#transformer-architecture","title":"Transformer Architecture","text":"<p>GPT-3 (Generative Pre-trained Transformer 3) breaks the NLP boundaries with training on 175B parameters. It is built on Transformer which use self-attention mechanism to weigh the significance of different words in a sentence to understand the context in a sequence of data.</p> <p>To process a text input with a transformer model, the text is tokenized into a sequence of words or part of words. These tokens are then encoded as numbers and converted into embeddings, which are vector-space representations of the tokens that preserve their meaning. Next, the encoder in the transformer transforms the embeddings of all the tokens into a context vector. Using this vector, the transformer decoder generates output based on clues. The decoder can produce the subsequent word. This process can be repeated to create an entire paragraph. This process is called auto-regressive generation.</p> <p>The attention mechanism computes the similarity between tokens (the embeddings of words) in a sequence. The process keeps few tokens around each word to help understand the context. This surrounding group of tokens is called the context window. It is the sliding group of tokens around a word that provides contextual information. That way, the model builds an intuition of what the text is saying. The closer two words are in a vector space, the higher the attention scores they will obtain and the higher the attention they will give to each other.</p> <p>The second part of the GPT-3 architecture is the set of layers of transformers stacked on top of each other. Within each layer, there are feed-forward neural networks to process the data.</p> <p>The training has two stages: Pre-training where the model attempts to predict the next word in a sentence using its own corpus, and fine tuning where the model can be tuned for specific tasks or content. During the pre-training process, the model automatically takes context into account from all the training data, and tracks relationships in sequential data, like the words in a sentence, to develop some understanding of the real world.</p> <p>At inference time, the input text is tokenized into individual tokens which are fed into the model. After processing using the transformer mechanism, the model returns result tokens which are then turned back into readable text.</p>"},{"location":"gen_ai_gs/#use-cases","title":"Use cases","text":"<p>We can group the Generative AI use cases in the following different categories:</p>"},{"location":"gen_ai_gs/#improve-customer-experiences","title":"Improve customer experiences","text":"<ul> <li>Chatbot functionality with context with better user experience than earlier technologies. </li> <li>Reduces operational costs using automated responses.</li> <li>Documentation summarization: See models like Jurassic-2 Jumbo from AI21 studio.  Anthropic's <code>claude-v2</code> works well too.</li> <li>Personalization</li> </ul>"},{"location":"gen_ai_gs/#improve-employee-productivity","title":"Improve employee productivity","text":"<ul> <li>Code generation</li> <li>Translation, reports, summarization...</li> <li> <p>Search via Q&amp;A Agent for specific subjects, based on enterprise document processing. The LLM helps understanding the text and the questions. The LLM is enriched and trained on a proprietary corpus:</p> <p></p> </li> <li> <p>Self service tutor based on student progress, prompt activities, and respond to questions</p> </li> <li>Personalized learning path generation</li> <li>Low-code development with GenAI agents</li> </ul>"},{"location":"gen_ai_gs/#creativity","title":"Creativity","text":"<ul> <li>Auto-generation of marketing material</li> <li>Personalized emails</li> <li>Sales scripts for customer's industry or segment</li> <li>Speeding the ideation phase of a product development</li> </ul>"},{"location":"gen_ai_gs/#business-process-optimization","title":"Business process optimization","text":"<ul> <li> <p>Automatically extract and summarize data from documents: combine OCR with prompt to extract data and build json doc to be structured for downstream processing: Gen AI based intelligent document processing may look like this:</p> <p></p> </li> <li> <p>Data augmentation to improve data set quality. Keep the privacy of original data sources, and help trains other models: generate image of rusted pumps to train an anomaly detection model on pumps.</p> </li> <li>Propose some supply chain scenario</li> </ul>"},{"location":"gen_ai_gs/#retrieval-augmented-generation-rag","title":"Retrieval Augmented Generation (RAG)","text":"<p>LLMs are trained at a specific moment in time, and therefore have a knowledge cut-off time, after which data are not directly available to the model.  When enterprises need their LLM-based applications to use private information, they can fine-tune models or insert semantic search results into the LLM's input context window. Retrieval Augmented Generation is an approach that addresses this problem by supplementing generative text models with data that it was not trained on but is discovered by examining the user's query and finding relevant additional information prior to calling the LLM.</p>"},{"location":"gen_ai_gs/#basic-architecture","title":"Basic architecture","text":"<p>The Retrieval Augmented Generation may be seen as a three stage process:</p> <p></p> <ol> <li>Indexing: A batch process to ingest documents and data from a source and index them. During this process semantic search is used to retrieve relevant documents from the index. Indexing supports reading the documents and splitting large ones into smaller chunks. Chunks help to stay within the LLM's context window. Indexing includes storage of the documents and index the chunks.</li> <li>Retrieval: Retrieves the relevant data (chunks) from the index, then pass that to the model as part of the context.</li> <li>Generation: Generate the response in plain natural language.</li> </ol> <p>This process is supported by tools for documents ingestion, splitting, embedding, indexing, retrieval and integration with the real time conversation flow.</p> <p>RAG systems work well because LLMs has the in-context learning capability, which allows models to use previously unseen data to perform accurate predictions without weight training.</p> <p>As LLM increase the context window size over new release, RAG can add more new data to it.</p>"},{"location":"gen_ai_gs/#rag-challenges","title":"RAG Challenges","text":"<p>Naive RAG has severe limitations which has limited large-scale adoption:</p> <ul> <li>It is hard to do reliable, scalable RAG on a large knowledge corpus.</li> <li>Limited to single-shot prompts.</li> <li>No query understanding, just a sematic search.</li> <li>No query decomposition for planning.</li> <li>No tool use to query an external data source to enrich the context.</li> <li>No reflection and error correction to improve the quality of the response.</li> <li>No persistence or memory of previous queries.</li> </ul> <p>Therefore, while invaluable for enriching context, RAG often needs to be enhanced with other technologies and approaches for full-scale enterprise applications.</p>"},{"location":"gen_ai_gs/#hybrid-ai","title":"Hybrid AI","text":"<p>Hybrid AI refers to the integration of different AI approaches or techniques to create more robust and capable AI systems. The classical hybrid AI is to combine traditional, rule-based symbolic AI systems with deep learning models. The rule-based engine can handle logical reasoning and knowledge representation, while the deep learning component handles pattern recognition and unstructured data processing.</p> <p>In addition to rule-based systems, Hybrid AI solutions can also leverage other decision engines such as optimization engines, knowledge graphs, or synbolic planning engines for specialized domain-specific applications.   In addition, all of these can be enhanced with specialized machine learning models to provide additional contextual information about historical patterns and scores.</p> <p>Another architecture for hybrid AI a multi-agent system, which combines multiple specialized AI agents, each with their own capabilities and decision-making processes, to tackle complex problems that require diverse skills and perspectives. These agents can be chained together or deployed for parallel processing. Agents are stateful.</p> <p>Here is a typical example of a hybrid AI architecture:</p> <p></p> <p>The backend for a chatbot includes assistants that organize an agent workflow, that integrates LLMs, classical machine learning predictive scoring models, knowledge graphs, rule-based engines, and vector stores for similarity searches. Such an agent workflow is stateful.</p>"},{"location":"gen_ai_gs/#why-use-hybrid-ai","title":"Why Use Hybrid AI?","text":"<p>Hybrid systems can achieve better overall accuracy than a generic LLM; they are more resilient to the weaknesses or limitations of LLMs.  They are grounded, precise, transparent, and more amenable to meaningful explanations of their behavior and decisions.  They can take into account up-to-date data from documents and data within the enterprise. The use of knowledge graph and rule-based systems make the AI architecture more flexible to support a wider range of problems and scenarios.</p> <p>When an enterprise must demonstrate compliance and follow specific policies and regulations, symbolic AI results are more interpretable and explainable than neural network decisions.  This is critical both for good organizational behavior and for audits.</p>"},{"location":"gen_ai_gs/#hybrid-ai-conclusion","title":"Hybrid AI Conclusion","text":"<p>Of course, leveraging multiple technologies comes at a cost:  The architecture, development, and governance costs of maintaining such a solution is significant.   Typically symbolic engine solutions must be hand-coded (for example, business rules must be discovered, authored, tested, and deployed).   Machine learning models must be trained on large amounts of data.   RAG systems depend on a curated and processed corpus of documents.</p> <p>All of this is real work that requires people with skills and experience.   Tools exist and are being enhanced all the time to simplify the process of creating and maintaining these components, but enterprises should go into these projects with their eyes open to avoid frustration and disappointment.    AI can produce great results - but it's not magic.   Our recommendation:  work with people experienced not only the latest AI technologies, but in created real, deployed, enterprise systems, and people who have experience with the full breadth of tools that can be brought to bear to create a meaningful solution for your business.</p> <ol> <li> <p>We use the term query, but in reality the LLM generates the next most likely tokens after the content of the context window given its training model; no database or other external data source is queried per se.  Any such external reference is done by adding more data to the context window; techniques like tool calling iteratively add information to the context by calling 3<sup>rd</sup> party APIs.\u00a0\u21a9</p> </li> </ol>"},{"location":"methodology/","title":"Methodology to develop Neuro-Symbolic Solution","text":""},{"location":"methodology/#methodology-to-develop-neuro-symbolic-solution","title":"Methodology to develop Neuro-Symbolic Solution","text":""},{"location":"methodology/#understand-the-scope-of-the-application","title":"Understand the scope of the application","text":"<ul> <li>Review what is the goal of the assistant</li> <li>Review each system to gather information</li> </ul>"},{"location":"demos/","title":"Introduction","text":""},{"location":"demos/#demonstration-repository","title":"Demonstration repository","text":"<p>This repository includes pre-defined demonstrations that can be used with different decision services and different use cases, and instructions and tools to jump start your own solution.</p> Name Goal IBU Insurance A chatbot to help IBU Insurance\u2019s customer service representatives manage customer complaints IBU MiniLoan A demonstration of Agentic solution with IBM ODM Miniloan sample <p>To watch the IBU Insurance demonstration video  on YouTube.</p> <p></p> <p>If you want to develop your own demonstration or jump start your own solution see these instructions.</p>"},{"location":"demos/#contact-us","title":"Contact us","text":"<p>Athena is here to help you seamlessly integrate this innovative framework into your operations. For a customized proof of concept or production deployment, feel free to reach out to the expert team at Athena Decision Systems or email Athena. With deep industry knowledge and tailored solutions, Athena will ensure a smooth and successful implementation that drives tangible value for your business.</p>"},{"location":"demos/build_sol/","title":"Build Your Own Solution","text":""},{"location":"demos/build_sol/#build-solution","title":"Build solution","text":"Version <p>Created 06.2024 - Updated 07/10/24</p> <p>Creating a new solution using the owl framework should be straightforward at it uses yaml manifests at its core, but potential complexity may depend on the level of the requirements to integrate with external services. </p> <p>To get started consider the scope of the demonstration and assess if you need to:</p> <ul> <li>Use a specific LLM backend</li> <li>Have an existing decision service available to be used or if you need to develop a new one. A new decision service means new rules and a new data model for the rule processing. Rule discovery and analysis may take some time as the approach is to model knowledge from workers with rules and data model.</li> <li>The design of the tool calling may become more complex when the number of tool grow, and when some potential user's query may not be easy to map to tool.</li> </ul> <p>Recall that an Hybrid-AI solution includes as set of components working together to deliver more reliable results, higher accuracy with stateful persistence:</p> <p></p> <p>We recommend reading the design document, the architecture presentation and some tutorials to create agent.</p>"},{"location":"demos/build_sol/#jump-start-your-own-solution","title":"Jump start your own solution","text":"<p>As a first tutorial, we will use an existing decision service deployed to the IBM ODM decision server.</p>"},{"location":"demos/build_sol/#git-repositories","title":"Git repositories","text":"<p>We encourage you, to fork the OWL core repository https://github.com/AthenaDecisionSystems/athena-owl-core</p> <p></p> <p>And the demonstration repository: https://github.com/AthenaDecisionSystems/athena-owl-demos</p>"},{"location":"demos/build_sol/#pre-requisites","title":"Pre-requisites","text":"<p>The following tools and environments are needed:</p> <ul> <li>Python 3.11 or 3.12, using a virtual environment</li> <li>Get a the API keys for the different LLM you want to use in your solution: WatsonX.AI , OpenAI Anthropic, Mistral, ... and use the <code>/demo_tmpl/.env_tmpl</code> file to persist those API KEYS, rename the file as <code>.env</code> and move it into the demonstration folder you want to use or the new folder for your solution.</li> </ul> <pre><code>cp ../demo_tmpl/.env_tmpl .env\n</code></pre>"},{"location":"demos/build_sol/#create-project","title":"Create project","text":"<p>The following steps will soon be automatized with scripts and tools, but as of now, they are manual (sorry):</p> <ul> <li>Create a folder for your project: <code>IBM-MiniLoan-demo</code> in the athena-owl-demos folder. </li> <li>Copy the project template to the new folder:</li> </ul> <pre><code>cp -r demo_tmpl/ My-MiniLoan-demo\ncd My-MiniLoan-demo\n</code></pre> <ul> <li>Create and Start a Python virtual environment</li> </ul> <pre><code>python -m venv .venv\n# For windows PC\nsource .venv/Scripts/activate\n# For unix based PC\nsource .venv/bin/activate\n</code></pre> <p>The creation is needed only for the first time.</p> <ul> <li>Install the Python modules needed for the solution</li> </ul> <pre><code>pip -r ibu-backend/src/requirements.txt\n</code></pre>"},{"location":"demos/build_sol/#build-the-object-model-for-decision-service","title":"Build the object model for decision service","text":"<p>The decision service exposes an REST API with an Open API document. From this document it is easy to get the data model and create Python Pydantic objects from it. Here are the steps:</p> <ol> <li>Access the ODM Developer Edition console at http://localhost:9060/, select he decision server console</li> <li> <p>Navigate to the RuleApp &gt; Ruleset in the <code>Explorer</code> tab that you want to call:</p> <p></p> </li> <li> <p>Get the OpenAI yaml document and download it to a project folder <code>ibu_backend/src/openapi</code>.</p> <p></p> </li> <li> <p>Run the command to build the pydantic model</p> </li> </ol> <pre><code>datamodel-codegen  --input MydeploymentMiniloan_ServiceRulesetDecisionService.yaml --input-file-type openapi --output ../ibu/itg/ds/pydantic_generated_model.py\n</code></pre> <p>Below is an extract of the generated code, and an link to the code.</p> <pre><code>class Borrower(BaseModel):\n    name: Optional[str] = None\n    creditScore: Optional[int] = None\n    yearlyIncome: Optional[int] = None\n\n\nclass Loan(BaseModel):\n    amount: Optional[int] = None\n    duration: Optional[int] = None\n    yearlyInterestRate: Optional[float] = None\n    yearlyRepayment: Optional[int] = None\n    approved: Optional[bool] = True\n    messages: Optional[List[str]] = []\n</code></pre> <p>If needed review the input and optional fields as some empty attribute may generate null pointer exception in the rules For example approved need to be initialized to True and messages to be an empty array instead of None:</p> <pre><code>    yearlyRepayment: Optional[int] = None\n    approved: Optional[bool] = True\n    messages: Optional[List[str]] = []\n</code></pre> <p>Those steps need to be done each time there are changes to the ODM eXecutable Object Model. If this model is stable, those steps are done only one time.</p>"},{"location":"demos/build_sol/#build-the-python-function-to-call-odm","title":"Build the python function to call ODM","text":"LLM and Tool calling <p>OpenAI has started the initiative of function calling and now most of the major proprietary or open source LLM model supports tool calling. In the LLM tool calling mechanism the prompt is enhanced with information about the function signature. For example the LLM will see the following signature and will prepare the argument from the user's query </p><pre><code>def get_client_by_name(first_name: str, last_name: str)\n</code></pre> <p>Here is an example of LLM trace showing the preparation of the data: </p><pre><code>Invoking: `get_client_by_name` with `{'first_name': 'Robert', 'last_name': 'Smith'}`\n</code></pre> <p>In the code template <code>src/ibu/llm/tools/client_tools.py</code> define a new function to expose a set of parameters the LLM will be able to extract from unstructured query text:</p> <pre><code>def assess_loan_app_with_decision(loan_amount: int, duration: int,   first_name: str, last_name: str):\n    loanRequest= Loan(duration=duration, amount=loan_amount)\n    borrower =  build_or_get_loan_client_repo().get_client_by_name(first_name=first_name, last_name=last_name)\n    ds_request = Request(__DecisionID__= str(uuid.uuid4),borrower=borrower, loan=loanRequest)\n    payload: str = ds_request.model_dump_json()\n    return callRuleExecutionServer(payload)\n</code></pre> <p>The code needs to prepare the data to call IBM ODM, using the Pydantic objects created above. </p>"},{"location":"demos/build_sol/#define-tools","title":"Define Tools","text":"<p>The classical common integration is when the solution needs to get data from an existing database or better a microservice managing a specific business entity. In this case the solution leverage a python function that can remote call the microservice URL using library like <code>requests</code>.</p> <p>For short demonstration you may need to implement some mockup repository that could be integrated into the demo run time. The template folder includes such in memory repository. You need to update with your own data (See this file loanapp_borrower_repo_mock.py).</p> <pre><code>def initialize_client_db(self):\n        self.add_client(Borrower(name = \"robert dupont\",\n                            yearlyIncome = 50000,\n                            creditScore = 180\n                            ))\n    # ...\n</code></pre>"},{"location":"demos/build_sol/#adding-a-function-as-tool","title":"Adding a function as tool","text":"<p>As some Owl assistants are using LangGraph for agent orchestration, we will use LangChain tools API to define function calling.</p> <p>There are three ways to do so with LangChain: function annotation, using a factory function or class sub-classing. </p> <p>The tool annotation is the simplest approach. The following declaration uses annotation, and the argument names, type and comment description are very important as they will be injected as context in the prompt to the LLM. Be sure to be short but brings semantic so the LLM can decide which function to call and what parameters to extract such as the first and last names.</p> <pre><code>@tool\ndef get_client_by_name(first_name: str, last_name: str) -&gt; str | None:\n    \"\"\"get borrower client information given his or her name\"\"\"\n    return build_or_get_loan_client_repo().get_client_by_name_json(first_name,last_name)\n</code></pre>"},{"location":"demos/build_sol/#declaring-the-tool-in-yaml","title":"Declaring the tool in yaml","text":"<p>Update the <code>tools.yaml</code> file in the config folder:</p> <pre><code>ibu_client_by_name:\n  tool_id: ibu_client_by_name\n  tool_class_name: 'ibu.llm.tools.client_tools'\n  tool_description: 'get client information given his first and last name'\n  tool_fct_name: get_client_by_name\n</code></pre> Behind the scene <p>The tool factory implementation </p>"},{"location":"demos/build_sol/#define-prompt","title":"Define prompt","text":"<p>Prompts are defined in <code>prompts.json</code> file. </p>"},{"location":"demos/build_sol/#define-assistant","title":"Define Assistant","text":"<p>Add the following base declaration for the main Assistant of the solution. One Assistant per use case.</p> <pre><code>ibu_assistant:\n  assistant_id: ibu_assistant\n  class_name: athena.llm.assistants.BaseAssistant.BaseAssistant\n  description: A default assistant that uses LLM, and local defined tools like get borrower, and next best action\n  name: IBU Loan App assistant\n  agent_id: ibu_agent\n</code></pre> <p>The two important properties are the <code>class_name</code> and the <code>agent_id</code>.</p> <p>The BaseAssistant class name is coming from Owl Agent core library. </p> <p>This is the LangGraph flow with tool and LLM. The graph looks like in the following figure:</p> <p></p>"},{"location":"demos/build_sol/#define-agent","title":"Define Agent","text":""},{"location":"demos/build_sol/#integration-tests","title":"Integration tests","text":""},{"location":"demos/build_sol/#custom-user-interface","title":"Custom user interface","text":"<p>You can use the OWL Front End user interface as is and can slightly customize it via environment variables which can be set in the docker-compose file:</p> <pre><code>  owl-frontend:\n    hostname: owl-frontend\n    image: jbcodeforce/athena-owl-frontend:latest \n    container_name: owl-frontend\n    ports:\n      - 3000:80\n    environment:\n      - REACT_APP_OWL_AGENT_NAME=\"YOUR DEMO NAME\"\n      - REACT_APP_BACKEND_URL=http://localhost:8000/api/v1/\n      - REACT_APP_ASSISTANT_ID_WITH_RULES='ibu_assistant'\n      - REACT_APP_ASSISTANT_ID_WITHOUT_RULES='ibu_assistant_limited'\n      - REACT_APP_DEMO_TEXT=\"ONE SENTENCE to use for the demo\"\n</code></pre>"},{"location":"demos/build_sol/#troubleshooting","title":"Troubleshooting","text":"<p>Access to the logs of decision server or owl backend server by doing</p> <pre><code>docker logs owl-backend\ndocker logs decisionsvc\n</code></pre>"},{"location":"demos/build_sol/#exception-when-starting-odm-decision-server","title":"Exception when starting ODM decision server","text":"<p>The trace of the decision service may log an exception of sequence number already created. </p> <pre><code> org.h2.jdbc.JdbcSQLSyntaxErrorException: Sequence \"SYSTEM_SEQUENCE_AAD2612D_FF17_4435_A436_6D4A63BF6D6E\" already exists; SQL statement:\n</code></pre> <p>This may come from previous execution on a new database. Just deleting the <code>decisions/persistence</code> folder, and restarting the decision server solved the problem.</p>"},{"location":"demos/insurance/","title":"IBU Insurance","text":""},{"location":"demos/insurance/#ibu-insurance","title":"IBU Insurance","text":"Version <p>Created 05/2024. Updated 07/11/2024 - STILL UNDER WORK</p> <p>The IBU insurance demonstration illustrates the integration with data manager service, a decision service, a vector store and a LLM as shown in the figure below:</p> <p></p> <ul> <li> <p> Set up in 5 minutes</p> <p>create a <code>.env</code> file under IBU-insurance-demo with the API key for the LLM you want to use. See the file template in <code>demo_tmpl/.env_tmpl</code>. Then cd IBU-insurance-demo/deployment/local &amp;&amp; docker-compose up -d </p> <p>and get up and running in minutes. </p> <p> Demonstration Script</p> </li> <li> <p> Develop around this demo</p> <p>Tune the content of this demonstration or deep dive into how it is built.  Work on code</p> </li> <li> <p> Open Source, Apache</p> <p>Owl Agent and demonstration are licensed under Apache and available on GitHub</p> <p> License</p> </li> </ul>"},{"location":"demos/insurance/#goals","title":"Goals","text":"<p>The IBU Insurance agent chatbot helps IBU Insurance\u2019s customer service representatives manage customer complaints about their claims handling. The chatbot is used by the customer service reps when a customer calls or writes with a complaint.</p> <p>The chatbot should bring consistent responses and actionable decisions, which improves the complain management by more than 27% and improve the quality responses by 35% while reducing the cost by around 60%. </p> <p></p> <p>Link to video</p>"},{"location":"demos/insurance/#insurance-context","title":"Insurance context","text":"<p>In most insurance organization we may find the following roles Involved in Complaint Handling process:</p> <p></p> <p>The AI assistant will help the contact center agents.</p> <p>In the insurance industry the strategy to better manage customers is based on a metric called the Customer LifeTime Value or CLTV. The higher the value, the better will be the customer support, with some personalized service with dedicated advisor. At the lower range, the insurance may let their customers go away as it might actually reduce adverse selection and improve the overall profitability of the company. Finally for the bigger part of the customer profile, the company may want to retain them but using some retention effort at the minimum cost. </p> <p>As part of the automated chatbot integration the business policy may first evaluate the risk of churn and then reassign the interaction to the retention department if needed.</p>"},{"location":"demos/insurance/#build-for-demonstration","title":"Build for demonstration","text":"<p>For development purpose, build the DataManager microservice image:</p> Under the datamgt folder<pre><code>./build/buildImage.sh\n</code></pre> <p>The docker images for the DataManager microservice, the chatbot frontend and for the OWL Backend are available on docker hub.</p> <p></p> <p>The docker compose starts by downloading docker images from Docker Hub. Those images were built on intel based architecture. If you run on arm architecture like the MAC M family, you need to build the owl backend and owl front end images (See the instructions here). </p>"},{"location":"demos/insurance/#run-locally","title":"Run locally","text":"<ul> <li>Get a the API keys for the different LLM you want to use in your solution: WatsonX.AI , OpenAI Anthropic, Mistral, ... and use the <code>/demo_tmpl/.env_tmpl</code> file to persist those API KEYS, rename the file as <code>.env</code> and move it the miniloan demonstration folder.</li> </ul> <pre><code># under IBU-insurance-demo\ncp ../demo_tmpl/.env_tmpl .env\n</code></pre> <p>To start all the components of the solution like the owl_backend, the owl_frontend, the data manager, postgresql database, and the ODM decision service, use the docker compose file locally under the <code>IBU-insurance-demo/deployment/local/</code> folder. </p> <ul> <li>Start all the services:</li> </ul> IBU-insurance-demo/deployment/local/ folder.<pre><code>docker-compose up -d \n</code></pre> <p>The first time you launch it, it may take some time as it downloads the needed docker images from docker hub.</p> <ul> <li>Verify that the six containers are running:</li> </ul> <pre><code>docker ps\n</code></pre> <pre><code>2ecb23b78ad5   jbcodeforce/ibu-insurance-data-mgr:latest  0.0.0.0:8080-&gt;8080/tcp, 8443/tcp         datamgr\n3988ffd617c6   jbcodeforce/athena-owl-backend:latest      0.0.0.0:8000-&gt;8000/tcp                   owl-backend\n258460ed25ed   jbcodeforce/athena-owl-frontend:latest      0.0.0.0:3000-&gt;80/tcp                   owl-frontend\n349f3beb4174   icr.io/cpopen/odm-k8s/odm:8.12             0.0.0.0:9060-&gt;9060/tcp, 9080/tcp, 0.0.0.0:9443-&gt;9443/tcp, 9453/tcp   decisionsvc\n070e124923f7   postgres:latest                            0.0.0.0:5432-&gt;5432/tcp                   postgres\n86052092cfe7   ghcr.io/chroma-core/chroma:latest          0.0.0.0:8005-&gt;8000/tcp                   chroma-db\n</code></pre> <ul> <li>To look at the owl-backend logs</li> </ul> <pre><code>docker logs owl-backend\n</code></pre> <p>Next see the demonstration script section below, or the non-regression tests to validate automatically the demonstration scenario execution.</p>"},{"location":"demos/insurance/#demonstration-flows","title":"Demonstration Flows","text":"<p>The business policies are declared in a semi-structured document, and were extracted using the \"Agile Business Rule Development Methodology\". An extract of this document is shown in the figure below:</p> <p></p> <ul> <li>The business policy 52 is implemented in IBM ODM as the following rule:</li> </ul> <p></p> <p>Which is visible in the Decision Center at the address http://localhost:9060/decisioncenter.</p> <ul> <li>Now to illustrate that using the policy document as input for a Retrieval Augmented Generation will provide limited value to customer support staff's query, you can use the OWL Frontend user's interface at http://localhost:3000/:</li> </ul> <p></p> <p>[Optional]: Once the document is uploaded, you can use the OWL_backend APIs to do a some similarity searches on the document content. The URL for the APIs is http://localhost:8000/docs, while the API for similarity search is in the documents RESTful resource .</p> <ul> <li>Using the following query: \"\" we can see that the LLM with or without RAG does not give the real answer, and also at different time, it returns different answer</li> </ul> <p></p> <ul> <li>Setting the flag to use ODM, give the result according to the rules:</li> </ul> <p></p>"},{"location":"demos/insurance/#validate-the-demo-with-python-code","title":"Validate the demo with python code","text":"<p>Under the <code>e2e</code> folder you can find different tools to support automatic testing:</p> <ul> <li>Create python virtual environment if not created before:</li> </ul> <pre><code>python -m venv .venv\n# for MAC / Linux users\nsource ./venv/bin/activate\n# for Windows\nsource ./venv/Scripts/activate\n</code></pre> <ul> <li>Install needed library to run those tests</li> </ul> <pre><code>cd e2e\npip install -r requirements.txt\n</code></pre> <ul> <li>Run the happy path tests:</li> </ul> <pre><code>cd e2e\npython non_regression_tests.py\n</code></pre>"},{"location":"demos/insurance/#maintenance-activities-for-the-demonstration","title":"Maintenance activities for the demonstration","text":"<ul> <li>It may be relevant to reset the vector store database. To do so, delete the folder with the data content. </li> </ul> <pre><code>cd IBU-insurance-demo/deployment/local\nrm -r data/chromadb\nrm -r data/file_content\n</code></pre> <ul> <li>Restarting the owl-backend. As of now as the backend does not use a database, it loads Owl Entity definitions from file manifests. So sometime it is needed to restart the server using the following commands:</li> </ul> <pre><code>cd IBU-insurance-demo/deployment/local\ndocker stop owl-backend\ndocker-compose up -d   # should restart only the owl-backend\n</code></pre>"},{"location":"demos/insurance/#architecture","title":"Architecture","text":"<p>The high level the architecture for this demonstration looks like in the following figure:</p> <p></p> <ul> <li>A chatbot supports the interactions in natural language queries</li> <li>The assistant server manages the conversation and the integration with the different backends. There are two assistants defined for this demonstration, one using IBM ODM decision service, one without it. </li> </ul>"},{"location":"demos/insurance/#component-definitions","title":"Component Definitions","text":""},{"location":"demos/insurance/#define-the-assistant","title":"Define The Assistant","text":"<p>The assistant needs to do different operations to address queries in the insurance domain. The flow is introduced in the following diagram and implemented using LangGraph API:</p> <p></p> <p>The code supporting this graph is in the IBU_Assistant_LG.py and uses LangGraph API</p> <ul> <li>The first agent, <code>ibu_classify_query_agent</code>,  has a system prompt to assess if the query is about an information query or about a complaint related to an insurance entity. The tools used to support both paths are not the same. The system prompt is <code>classify_query_prompt</code>. The agent is  <code>ibu_classify_query_agent</code></li> </ul> <pre><code>graph.add_node(\"gather_information\", self.process_information_query)\ngraph.add_node(\"process_complaint\", self.process_complaint)\ngraph.add_node(\"activate_tools_for_info\", tool_node_1)\ngraph.add_node(\"activate_tools_for_complaint\", tool_node_2)\n</code></pre> <ul> <li> <p>The gathering information agent is using a prompt to search for information, using query tools to claim or client databases and then external web search. The agent is <code>ibu_tool_rag_agent_limited</code>.</p> </li> <li> <p>The complaint processing agent is <code>ibu_tool_rag_agent</code>.</p> </li> <li>The active tool nodes are taking the output for the LLM to assess the tool selections and then execute those tools, accumulates the responses to send them back to the matching agent. The tool lists are different in each agent:</li> </ul> agents.yaml file in config folder<pre><code>ibu_tool_rag_agent_limited:\n  agent_id: ibu_tool_rag_agent_limited\n  tools:\n    - ibu_client_by_id\n    - ibu_client_by_name\n    - ibu_claim_by_id\n    - tavily\n\n# \nibu_tool_rag_agent:\n   agent_id: ibu_tool_rag_agent\n   tools:\n    - ibu_best_action\n    - ibu_client_by_id\n    - ibu_client_by_name\n    - ibu_claim_by_id\n</code></pre> <p>To navigate in the graph conditions are added to the edges:</p> conditonal edge in IBU_Assistant_LG.py<pre><code># from the gathering information node, if there is a tool call in the response then goes to activate the tools\n graph.add_conditional_edges(\n    \"gather_information\",  # (1)\n    self.route_tools,      # (2)\n    { \n        \"tools\": \"activate_tools_for_info\", \n        END: END\n    }\n )\n</code></pre> <ol> <li> The node with LLM</li> <li> The function to decide where to route</li> </ol> <p>The route tools function looks at the present of tool_calls in the LLM ai message response:</p> route_tools function<pre><code> if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) &gt; 0:\n</code></pre>"},{"location":"demos/insurance/#define-agents","title":"Define Agents","text":""},{"location":"demos/insurance/#ibu_classify_query_agent","title":"ibu_classify_query_agent","text":"<p>The first agent declaration uses the <code>IBUClassifyQueryAgent</code> class.</p> File ibu_backend/src/config/agents.yaml<pre><code>ibu_classify_query_agent:\n  agent_id: ibu_classify_query_agent\n  name: ibu_classify_query_agent\n  description: openai based agent with simple prompt \n  class_name: ibu.llm.agents.ClassifyQueryAgent.IBUClassifyQueryAgent\n  modelClassName: langchain_openai.ChatOpenAI\n  modelName: gpt-3.5-turbo-0125\n  prompt_ref: classify_query_prompt\n  temperature: 0\n  top_k: 1\n  top_p: 1\n</code></pre>"},{"location":"demos/insurance/#ibu_tool_rag_agent","title":"ibu_tool_rag_agent","text":"<p>To illustrate the integration with Vector Store a specific agent: tool_rag_agent is defined.</p> File ibu_backend/src/config/agents.yaml<pre><code>ibu_tool_rag_agent:\n  agent_id: ibu_tool_rag_agent\n  name: ibu_tool_rag_agent\n  description: openai based agent with prompt with query and context\n  class_name: ibu.llm.agents.tool_rag_agent.IbuToolRagAgent\n  modelClassName: langchain_openai.ChatOpenAI\n  modelName: gpt-3.5-turbo-0125\n  prompt_ref: ibu_rag_prompt\n  temperature: 0\n  top_k: 1\n  top_p: 1\n  tools:\n    - ibu_best_action\n    - ibu_client_by_id\n    - ibu_client_by_name\n    - get_claim_status_by_user_name\n    - ibu_claim_by_id\n</code></pre>"},{"location":"demos/insurance/#ibu_tool_rag_agent_limited","title":"ibu_tool_rag_agent_limited","text":"<p>Using the same code as above but with other tools.</p> File ibu_backend/src/config/agents.yaml<pre><code>ibu_tool_rag_agent_limited:\n  agent_id: ibu_tool_rag_agent_limited\n  name: OpenAI IBU agent with RAG Limited\n  description: OpenAI IBU agent with RAG and insurance tool without decision service\n  class_name: ibu.llm.agents.tool_rag_agent.IbuToolRagAgent\n  modelClassName: langchain_openai.ChatOpenAI\n  modelName: gpt-3.5-turbo-0125\n  prompt_ref: ibu_rag_prompt\n  temperature: 0\n  top_k: 1\n  top_p: 1\n  tools:\n    - ibu_client_by_id\n    - ibu_client_by_name\n    - get_claim_status_by_user_name\n    - ibu_claim_by_id\n    - tavily\n</code></pre>"},{"location":"demos/insurance/#define-tools","title":"Define Tools","text":"<p>This is one of the most important exercise. The tools are functions and declarations in manifest file. </p> <p>The get client information by using the first and last names is a function that uses a HTTP client to call the DataManager microservice, using the repository pattern:</p> client_tools.py<pre><code>def get_client_by_name(firstname: str, lastname: str) -&gt; dict:\n    \"\"\"get client information given his or her name\"\"\"\n    return build_or_get_insurance_client_repo().get_client_by_name(firstname, lastname)\n</code></pre> <p>The repository is a Singleton. </p> Declaration in config/tools.yaml<pre><code>ibu_client_by_name:\n  tool_id: ibu_client_by_name\n  tool_name: \"Client by lastname and firstname\"\n  tool_class_name: 'ibu.llm.tools.client_tools'  # (1)\n  tool_description: 'get client information given his or her lastname and firstname'\n  tool_fct_name: get_client_by_name\n</code></pre> <ol> <li>The module file name where the function is implemented</li> </ol>"},{"location":"demos/insurance/#define-instructions","title":"Define instructions","text":"<p>System prompts are defined externally in the prompts.yaml and then referenced in the agent:</p> IBU Classify Query Agent in agents.yaml<pre><code>ibu_classify_query_agent:\n  agent_id: ibu_classify_query_agent\n  name: IBU Classify Query Agent\n  prompt_ref: classify_query_prompt  # (1)\n</code></pre> <ol> <li>Reference to a system prompt.</li> </ol> <p>The matching prompt:</p> Classify the query instructions in prompts.yaml<pre><code>classify_query_prompt:\n  prompt_id:  classify_query_prompt\n  name: Classify the query instructions\n  locales:\n  - locale: en\n    text: &gt;\n      \"You are an expert at extracting intent from user question. Assess if this is a complaint or an information gathering query.\n        The information contains documents related to insurance policies, claim, coverages.\n        Use complaint for questions on insurance claim status or reimbursement or unhappy customer. Only return information or complaint.\"\n</code></pre>"},{"location":"demos/insurance/#development-activities","title":"Development activities","text":""},{"location":"demos/insurance/#prepare-a-virtual-environment","title":"Prepare a virtual environment","text":"<pre><code>python -m venv .venv\n# for MAC / Linux users\nsource ./venv/bin/activate\n# for Windows\nsource ./venv/Scripts/activate\n</code></pre> <p>To run unit tests or integration tests, once the Python virtual environment is started be sure to set the PYTHONPATH environment variable so the athena core module can be found:</p> <pre><code># under ibu_backend\nsource setpython.sh\n</code></pre>"},{"location":"demos/insurance/#run-unit-tests","title":"Run unit tests","text":"<p>All the unit tests are defined with Python unittests library.</p> <pre><code># under ibu_backend\npytest -s tests/ut/\n</code></pre>"},{"location":"demos/insurance/#debug-a-unit-tests","title":"Debug a unit tests","text":"<p>The following instructions are for VScode:</p> <ul> <li>Select the test class as the python file to execute</li> <li> <p>Start the debug and verify the launch configuration includes:</p> <pre><code>\"configurations\": [\n\n    {\n        \"name\": \"Python Debugger: Current File Demos\",\n        \"type\": \"debugpy\",\n        \"request\": \"launch\",\n        \"program\": \"${file}\",\n        \"console\": \"integratedTerminal\",\n        \"env\": { \"PYTHONPATH\": \"${workspaceRoot}/../athena-owl-core/owl-agent-backend/src\"},\n        \"cwd\": \"${workspaceRoot}/IBU-insurance-demo/ibu_backend\"\n    }\n]\n</code></pre> </li> </ul>"},{"location":"demos/insurance/#physical-deployment-of-the-demonstration","title":"Physical deployment of the demonstration","text":"<p>As of now the owl-backend is a container image, deployable as a unit and being  able to mount the python code of the demonstration to run the different orchestration logic. The diagram illustrates those concepts to run on a local machine with the data manager microservice running in its own container</p> <p></p> <p>For production deployment the owl-backend code and the specific logic may be packaged in its own container.</p>"},{"location":"demos/miniloan/","title":"IBM-ODM LoanApp","text":""},{"location":"demos/miniloan/#ibm-miniloan-odm-demonstration-with-agent","title":"IBM Miniloan ODM demonstration with Agent","text":"Version <p>Created 06.2024</p>"},{"location":"demos/miniloan/#goals","title":"Goals","text":"<p>The Miniloan application is part of the IBM Operational Decision Management product and tutorial. The goal of this hybrid AI demonstration is to illustrate how unstructured query in natural language can be decomposed using a LLM to identify parameters to call the loan application decision service and how to do it with the Owl Framework.</p> <p>The end-user's questions that are tested and validated are:</p> <ul> <li>What is the credit score of Robert Smith?: this will demonstrate call to a backend CRM database.</li> <li>My client robert smith wants to borrow $1,000,000 for 180 months  with a yearly repayment of $60,000 do you think it is possible?: will illustrate the call to the miniloan ruleapp using the loan parameters extracted by the LLM.</li> <li>One of our client, Jean Martin, wants a loan for $300,000 for a duration of 180 months and a yearly repayment of $40,000 do we approve it?, just to demonstrate that different rules apply.</li> </ul>"},{"location":"demos/miniloan/#architecture","title":"Architecture","text":"<p>The high level the architecture for this demonstration looks like in the figure below:</p> <p></p> <ul> <li>A chatbot supports the interactions with a customer support representative using natural language queries</li> <li>The assistant server manages the conversation and the integration with different backends. There are two assistants defined for this demonstration, one using IBM ODM MiniLoan decision service, one without it.</li> <li>The Loan App Decision service is the SAMPLE RuleApp deployed in a Rule Execution Server</li> <li>The different microservices to access the client database as future or existing borrowers, and to access the loan applications repository.</li> <li>The LLM is an externally Large Language Model accessible via API. Different models can be used.</li> </ul> <p>To make it easier the loanApp and client the repositories are mockup and loaded in memory.</p> <p>Recall that the rules implemented validate the data on the loan and borrower attributes:</p> <ul> <li>The max amount of money a loan could be:</li> </ul> <p></p> <ul> <li>The minimum credit score the borrower needs to have:</li> </ul> <p></p> <ul> <li>The yearly payment compare to the borrower incomes and credit score as a decision table:</li> </ul> <p></p> <p>Those rule evaluations are key to get reliable answer even if we have to process unstructured text.</p> <p>For more detail on the OWL Core components design see this note.</p>"},{"location":"demos/miniloan/#physical-deployment-of-the-demonstration","title":"Physical deployment of the demonstration","text":"<p>As of now the owl-backend is a container image, deployable as a unit and ables to mount the python code of the demonstration to run the different orchestration logic. The diagram illustrates those concepts to run on a local machine</p> <p></p> <p>For production deployment the owl-backend code and the specific logic may be packaged in its own container.</p>"},{"location":"demos/miniloan/#demonstration-flows","title":"Demonstration Flows","text":"<ul> <li>Get a the API keys for the different LLM you want to use in your solution: WatsonX.AI , OpenAI Anthropic, Mistral, ... and use the <code>/demo_tmpl/.env_tmpl</code> file to persist those API KEYS, rename the file as <code>.env</code> and move it the miniloan demonstration folder.</li> </ul> <pre><code># under IBM-miniloan-demo\ncp ../demo_tmpl/.env_tmpl .env\n</code></pre> <ul> <li>Start the docker compose with all the components of the above architecture.</li> </ul> <pre><code>cd IBM-MiniLoan-demo/deployment/local/\ndocker compose up -d\n</code></pre> <ul> <li>The Frontend user interface is available at http://localhost:3000/, clicking to the gear will bring the setting panel:</li> </ul> <ul> <li>The backend APIs is available at the following URL http://localhost:8000/docs. You do not need to use it for the demonstration scenario but feel free to use to it to understand the OWL entity model.</li> </ul> <ul> <li>Run a demonstration script to validate the deployment and the integration with LLM:</li> </ul> <pre><code># under the e2e folder\npython non_regression_tests.py\n</code></pre> <p>The script validates:</p> <ul> <li>The health if the server end-point</li> <li>Get the default system prompt designed for the demonstration</li> <li>Get the main assistant entity (the metadata about the assistant)</li> <li>Get the loan agent entity</li> <li>Get one of the tool entity to get information from a backend database</li> <li>Makes a unstructured query to ask about one of the existing client: What is the credit score of Robert Smith using IBU loan database?</li> <li>Assess a loan for a borrower with low credit score</li> <li>Assess for a successful loan application</li> </ul>"},{"location":"demos/miniloan/#demonstrating-with-the-user-interface","title":"Demonstrating with the User Interface","text":"<p>The User interface may be used to do a live demonstration:</p> <p>The following figure is using the <code>ibu_assistant_limited</code> assistant to try to answer the question. The tool calling to access the client data is successful but the LLM generates hallucinations:</p> <p></p>"},{"location":"demos/miniloan/#agentic-with-rule-engine","title":"Agentic with Rule Engine","text":"<p>This section explains how to use the OWL framework to support the demonstration. </p> <p>An assistant supports a customer representative to answer questions and queries about a loan. Assistant entity uses an agent, which is linked to the LLM to call via API and the tool definitions.</p> <p></p> <ul> <li>The assistant definition is simple and uses the BaseAssistant class from the Owl framework which uses LangChain's chain construct with or without tools definition.</li> </ul> <pre><code>ibu_assistant:\n  assistant_id: ibu_assistant\n  class_name: athena.llm.assistants.BaseAssistant.BaseAssistant\n  description: A default assistant that uses LLM, and local defined tools like get borrower, and next best action\n  name: IBU Loan App assistant\n  agent_id: ibu_agent\n</code></pre> <p>The assistant without ODM decision service function is:</p> <pre><code>ibu_assistant_limited:\n  assistant_id: ibu_assistant_limited\n  class_name: athena.llm.assistants.BaseAssistant.BaseAssistant\n  description: A default assistant that uses LLM, and local defined tools like get borrower, without decision service\n  name: IBU Loan App assistant\n  agent_id: ibu_agent_limited\n</code></pre> <ul> <li>The agent entity definition lists the prompt, and tools to use, and the LLM model. The <code>langchain_openai.ChatOpenAI</code> class is part of the langchain library. So a class that wraps a specific LLM API can be used.  </li> </ul> <pre><code>ibu_agent:\n  agent_id: ibu_agent\n  name: ibu_agent\n  description: openai based agent with IBU loan app prompt and tools\n  class_name: athena.llm.agents.base_chain_agent.OwlAgent\n  modelName: gpt-3.5-turbo-0125\n  modelClassName: langchain_openai.ChatOpenAI\n  prompt_ref: ibu_loan_prompt\n  tools:\n  - ibu_client_by_name\n  - ibu_loan_assessment_action\n</code></pre> <ul> <li>Tool definition looks to access data about the borrower/ client references the class that implement the tool function. The description is very important and represents a system prompt the LLM will use in the context window to infer the good data extraction from the unstructured query.</li> </ul> <pre><code>ibu_client_by_name:\n  tool_id: ibu_client_by_name\n  tool_class_name: 'ibu.llm.tools.client_tools'\n  tool_description: 'get client information given his or her name'\n  tool_fct_name: get_client_by_name\n</code></pre> <ul> <li>For the decision service, the same approach with a different function name.</li> </ul> <pre><code>ibu_loan_assessment_action:\n  tool_id: ibu_loan_assessment_action\n  tool_class_name: ibu.llm.tools.client_tools\n  tool_description: 'perform the loan application request assessment for the given borrower name'\n  tool_fct_name: assess_loan_app_with_decision\n</code></pre> <p>Assistants, agents and prompts are declarative. Tools need declaration but some code to do the integration. </p>"},{"location":"demos/miniloan/#development-around-the-demonstration","title":"Development around the demonstration","text":"<p>In case you need to work on the current demonstration, and run some of the test cases, this section addresses what needs to be done to run on you own local laptop (or a VM on the cloud) with a Docker engine. Currently, in development mode, the source code of the core framework is needed so you need to clone the github repository ( in the future, we may build a module that should be installable via <code>pip install</code>).</p> <pre><code># for example in $HOME/Code/Athena\n\ngit clone https://github.com/AthenaDecisionSystems/athena-owl-core\n</code></pre>"},{"location":"demos/miniloan/#unit-tests","title":"Unit tests","text":"<ul> <li>Define the PYTHONPATH so the core modules can be accessed during the tests executions:</li> </ul> <pre><code>    export PYTHONPATH=$WHERE_YOUR_CODE_IS/athena-owl-core/owl-agent-backend/src\n</code></pre> <ul> <li>Install specific library for testing</li> </ul> <pre><code># under ibu-backend\npip install -r tests/requirements.txt\n</code></pre> <ul> <li>Run all unit tests for the Miniloan:</li> </ul> <pre><code># under ibu-backend\npytest -s tests/ut\n</code></pre>"},{"location":"demos/miniloan/#integration-tests","title":"Integration tests","text":"<p>For integration tests, you need to start the backend using Docker Compose as explained before, then run all the integration tests via the command:</p> <pre><code># under ibu-backend\npytest -s tests/it\n</code></pre>"},{"location":"demos/miniloan/#code-explanations","title":"Code Explanations","text":"<p>The previous section demonstrates the Yaml manifests for the declaration of the assistant, agent and tools. Each demonstration will have different tools. This section explains the tools implemented in this demonstration which may help for future development.</p>"},{"location":"demos/miniloan/#code-structure","title":"Code structure","text":"<p>Each demonstration is its own folder and includes mostly the same structure:</p> <p></p> Folder Intent decisions/persistence Include the resDB ruleApps from IBM ODM deployment Infrastructure as code and other deployment manifests e2e end to end testing for scenario based testing with all components deployed ibu_backend/src The code for the demo ibu_backend/openapi Any openAPI documents that could be used to generate Pydentic objects. ODM REST ruleset openAPI is used. ibu_backend/config The different configuration files for the Agentic solution entities ibu_backend/tests Unit tests (ut folder) and integration tests (it folder)"},{"location":"demos/miniloan/#focusing-on-tools","title":"Focusing on tools","text":"<p>The tool function coding is done in one class, the client_tools.py. This class implements the different functions to be used as part of the LLM orchestration, and also the factory to build the tool for the LangChain framework.</p> <p>Taking the tool definition below</p> <pre><code>ibu_loan_assessment_action:\n  tool_id: ibu_loan_assessment_action\n  tool_class_name: ibu.llm.tools.client_tools\n  tool_description: 'perform the loan application request assessment for the given borrower name'\n  tool_fct_name: assess_loan_app_with_decision\n</code></pre> <p>The module <code>ibu.llm.tools.client_tools</code> includes the function <code>assess_loan_app_with_decision</code> that exposes the parameters the LLM can extract from the unstructured text and gives back to the langchain chains to perform the tool calling. The code prepares the payload to the ODM service.</p>"},{"location":"demos/miniloan/#future-enhancement","title":"Future enhancement","text":"<p>It is clear that the unstructured query is key to get good tool calling. It is possible to add a ODM rule set for data gathering. Any required data attributes needed to decide on the loan application may be evaluate from what the LLM was able to extract. LLM can build json payload conforms to a schema. Adding an agent to look at the data of the json, will help asking the question to the human to get all the missing data.</p> <p></p>"},{"location":"demos/miniloan/#next","title":"Next","text":"<p>As a next step you can study how to create your own demonstration &gt;&gt;&gt;</p>"},{"location":"design/design_multi_app/","title":"Owl Backend to support multiple demo","text":""},{"location":"design/design_multi_app/#owl-backend-to-support-multiple-demo","title":"Owl Backend to support multiple demo","text":"<p>For getting multiple demos in the same backend we can add the concept of apps.</p> <p>In the docker image the owl_backend can be organized as</p> <p>/owl-backend/athena   -&gt; backend source code /owl-backend/apps/base/config   -&gt; folder with the current config folder content from src/athena/config /owl-backend/apps/ibu-insurance/config  -&gt; folder for the specific configuration of the agents, tools, prompt /owl-backend/apps/ibu-insurance/src   -&gt; specific python code for tools and integration  /owl-backend/apps/ibm-loan-demo/config /owl-backend/apps/ibm-loan-demo/src</p> <p>When the owl-backend starts it can list the content of /owl-backend/apps folders to see the apps present and merge the agents, tools, and prompts definitions. For the code to be executable we need to set the PYTHONPATH to the demo code src.</p> <p>The code and config for a demos still come from a dedicated repostiory.  To run the demo locally we contiue to mount code and config inside the docker container.</p> <p>Once deployed on k8s we can copy the content of the demo to /owl-backend/apps/ via kubectl cp</p>"},{"location":"design/keycloak_support/","title":"A set of notes on how to integrate Keycloak","text":""},{"location":"design/keycloak_support/#a-set-of-notes-on-how-to-integrate-keycloak","title":"A set of notes on how to integrate Keycloak","text":""},{"location":"design/keycloak_support/#requirements","title":"Requirements","text":"<ul> <li>Be able to lock access to certain API if there is no valida token</li> <li>Be able to use Swagger UI still</li> <li>Be able to run unit and integration tests</li> </ul> <p>OAuth2 is a specification that defines several ways to handle authentication and authorization. OpenID Connect is another specification, based on OAuth2, to make it more interoperable.</p> <p>OpenAPI defines the <code>apiKey</code>, <code>http</code>, <code>oauth2</code>, <code>openIdConnect</code> security schemes. Http with a bearer: a header Authorization with a value of Bearer plus a token</p>"},{"location":"design/keycloak_support/#keycloak-summary","title":"Keycloak summary","text":"<p>Keycloak provides user federation, strong authentication, user management, fine-grained authorization. The access token issued by Keycloak is in JWT format. It consists of a header, a payload, and a signature separated by periods.</p> <p>A realm in Keycloak is equivalent to a tenant. Each realm allows an administrator to create isolated groups of applications and users</p>"},{"location":"design/keycloak_support/#fastapi-and-keycloak","title":"FastAPI and Keycloak","text":"<p>There are different libraries available. We can try to use the bare minimum and leverage FastAPI. FastAPI ways to support authentication and authorization.. FastAPI provides several tools for each of the security schemes in the <code>fastapi.security</code> module that simplify using these security mechanisms.</p> <p>The validity of the token is established by checking:</p> <p>The token\u2019s signature. The content of the claims.</p>"},{"location":"tutorials/dev_env/","title":"Environment Setup","text":""},{"location":"tutorials/dev_env/#setting-up-your-development-environment","title":"Setting up your development environment","text":"<p>This section describe how to use the Owl Agent Framework to start developing new assistant and work on the core components or demos we are providing.</p>"},{"location":"tutorials/dev_env/#one-time-setup","title":"One time setup","text":"<p>If you want to contribute to the Core Framework, you need to fork the current  https://github.com/AthenaDecisionSystems/athena-owl-core repository to your own account and then clone your repository.</p> <p></p> <p>If you do not want to participate, you can close the core repository to access some of the scripts and base code:</p> <ul> <li>Clone the core framework code and tools</li> </ul> <pre><code>git clone https://github.com/AthenaDecisionSystems/athena-owl-core\n# or your forked repo\ngit clone https://github.com/&lt;GIT_USER&gt;/athena-owl-core\n</code></pre> <ul> <li>[Optional] clone the demonstration repository: https://github.com/AthenaDecisionSystems/athena-owl-demos.</li> </ul> <pre><code>git clone https://github.com/AthenaDecisionSystems/athena-owl-demos\n</code></pre> <p>Again, if you want to contribute to the demonstration repository, you should first fork it and then clone your forked repo.</p> <ul> <li>You need Python 3.11 or 3.12</li> <li>Create (one time) and Start (each time you want to start working on the code) a Python virtual environment:</li> </ul> <pre><code>python -m venv .venv\n# For windows PC\nsource .venv/Scripts/activate\n# For unix based PC\nsource .venv/bin/activate\n</code></pre> <ul> <li>Within the Python virtual environment, install the Python module for core backend:</li> </ul> <pre><code>cd owl-agent-backend\n# under the folder owl-agent-backend\npip install -r src/requirements.txt\n</code></pre> <p>And for the testing </p> <pre><code># under the folder owl-agent-backend\npip install -r tests/requirements.txt\n</code></pre> <ul> <li>Create a .env file for accessing remote LLM, and update any of the API KEY. </li> </ul> <pre><code># under the folder owl-agent-backend\ncp ../tools/.env_tmpl .env\n</code></pre> <p>You need at least the Mistral and OpenAI keys</p> <pre><code>OPENAI_API_KEY=---your-key---\nWATSONX_APIKEY=---your-key---\nMISTRAL_API_KEY=---your-key---\nLANGCHAIN_API_KEY=---your-key---\nLANGCHAIN_TRACING_V2=true\nLANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n...\n</code></pre>"},{"location":"tutorials/dev_env/#validating-unit-tests-for-the-backend","title":"Validating Unit Tests for the Backend","text":"<p>To verify your environment is running file for the core components, run all unit tests</p> <pre><code>pytest -s tests/ut/\n</code></pre> <p>Most of the tests are done using Python unittest and pytest</p>"},{"location":"tutorials/dev_env/#understand-the-core-framework-code-organization","title":"Understand the Core Framework code organization","text":"<p>The following figure presents the backend code organization:</p> <p></p> Folder Functions routers Includes the different REST resources: conversation, assistant, agent, tool, prompt and document config Configuration file when the application runs in container. The configuration files are mounted inside the container under /app itg This is for the integration, so most of the code there is mockup placeholder for demonstration, the only important component is the store to keep data about the files uploaded in the context of RAG llm The different repository to manage each OwlEntities like, assistant, agent, tools, prompts...There are also some pre-defined assistants and agents that could be used for integrating in solution <p>The config folder and llm folder are the one you may add code and configuration into. For specific end-to-end demo we recommend using a separate repository and the start your own solution tool.</p>"},{"location":"tutorials/dev_env/#running-locally","title":"Running locally","text":""},{"location":"tutorials/dev_env/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>build the owl backend image under the <code>owl-agent-backend</code> folder.</li> </ul> <pre><code>./build/buildImage.sh\n</code></pre> <p>This image can be pushed to docker hub. (as of now it is the <code>jbcodeforce/athena-owl-backend:latest</code> image)</p> <p></p> <ul> <li>Build the owl-frontend docker image under the <code>owl-agent-frontend</code> folder:</li> </ul> <pre><code>./build/buildImage.sh\n</code></pre>"},{"location":"tutorials/dev_env/#development-mode","title":"Development mode","text":"<p>While developing the backend, the approach is to use test-driven development and start by writing unit tests for any new or changed feature. Each unit test python file defines a <code>unittest.TestCase</code> class and a test method. For each entity manager, instantiate a manager and then do some testing of the api.</p> <p>For conversations, use the ConversationControl object.</p> <pre><code>def test_base_assistant_with_chat_history(self):    \n    cc = ConversationControl()\n    cc.assistant_id=\"base_assistant\"\n    cc.user_id=\"unit_test\"\n    cc.thread_id=\"1\"\n    cc.chat_history=[]\n    cc.query=\"Hi, I'm Bob and my last name is TheBuilder.\"\n    rep = get_or_start_conversation(cc)\n    assert rep\n    assert rep.message\n</code></pre> <p>It is also possible to start a uvicorn server with continuous upload of the code and then test using the OpenAI exposed.</p> <ol> <li>Start the server under the <code>src</code> folder with <code>./start_backend.sh</code>. It uses a special config file (named local-config.yaml) to access the other entities configurations.</li> <li>Use the localhost:8000/docs URL</li> <li>Use any of the entity APIs</li> <li>Use the generic/chat URL to send the conversation.</li> </ol> <p>The minimum payload to use one of the assistant is the following:</p> <pre><code>{\n  \"query\": \"What is Athena Decision Systems all about?\",\n  \"user_id\": \"jerome\",\n  \"assistant_id\": \"fake_assistant\",\n  \"thread_id\": \"1\"\n}\n</code></pre>"},{"location":"tutorials/dev_env/#run-unit-tests","title":"Run unit tests","text":"<p>Running all the tests for non-regression validation:</p> <pre><code>pytest -s tests/ut/\n</code></pre> <p>To debug unit tests in VSCode:</p> <ul> <li>As the src code and tests are in separate folders, be sure to have configured the debugger launch settings as:</li> </ul> <pre><code>{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n\n        {\n            \"name\": \"Python Debugger: Current File\",\n            \"type\": \"debugpy\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"console\": \"integratedTerminal\",\n            \"env\": { \"PYTHONPATH\": \"${workspaceRoot}/owl-agent-backend/src\"},\n            \"cwd\": \"${workspaceRoot}/owl-agent-backend\"\n        }\n    ]\n}\n</code></pre> <p>Then breakpoint and step by step.</p> <p></p>"},{"location":"tutorials/dev_env/#integration-mode","title":"Integration mode","text":"<p>There are two ways to do integration tests, with docker or with the start_backend.sh script.</p>"},{"location":"tutorials/dev_env/#next","title":"Next","text":"<ul> <li>Implementing your own agent tutorial</li> </ul>"},{"location":"tutorials/new_mistral_agent/","title":"Building New Mistral Agent","text":""},{"location":"tutorials/new_mistral_agent/#developing-a-new-agent-based-on-mistral","title":"Developing a new agent based on Mistral","text":"<p>In this tutorial, you will learn how to develop a new agent for the Mistral API and integrate it into an existing Assistant. The end game code we will be developing can be found in <code>llm/agents/mistral_agent.py</code> and the config file.</p> <p>The following diagram illustrates what is built:</p> <p></p>"},{"location":"tutorials/new_mistral_agent/#pre-requisites","title":"Pre-requisites","text":"<p>You need to get a <code>MISTRAL_API_KEY</code> from mistral.ai. Be sure to get a little familiar with their API using their getting started guide.</p> <p>Update the <code>.env</code> file in the backend with the <code>MISTRAL_API_KEY</code> environment variable.</p> <p>You need to set up your development environment according to this note</p>"},{"location":"tutorials/new_mistral_agent/#defining-the-agent","title":"Defining The Agent","text":"<p>An agent defines at a minimum the LLM API client code, a model reference and parameters, and how to manage a conversation. A new agent has a contract with the OwlAgent Framework via the OwlAgent class. Therefore the first step is to create an agent Python file named <code>mistral_agent.py</code> and define a class that supports the contract of the constructor as shown below:</p> llm/agents/mistral_agent.py<pre><code>from athena.llm.agents.agent_mgr import OwlAgentInterface,\n\nclass MistralAgent(OwlAgentInterface):\n\n    def __init__(self,agentEntity: OwlAgentEntity, prompt: BasePromptTemplate, tool_instances: Optional[list[Any]]):\n        self.prompt = prompt\n        self.model=self._instantiate_model(agentEntity.modelName, agentEntity.modelClassName, agentEntity.temperature)\n</code></pre> <p>Since we are good test-driven developers, let's start by writing unit tests for this class: Under the <code>tests/ut</code> folder add a Python file called <code>test_mistral_agent.py</code> by copying the unit test template:</p> <pre><code># under tests/ut\ncp template_ut.py test_mistral_agent.py\n</code></pre> <p>Rename the class and add a first test:</p> tests/ut/test_mistral_agent.py<pre><code>from athena.llm.agents.agent_mgr import get_agent_manager, OwlAgentEntity\n\nclass TestMistral(unittest.TestCase):\n\n    def test_define_agent_entity_create_instance(self):\n        \"\"\"\n        From the OwlAgentEntity verify the factory can create the agent executor\n        \"\"\"\n        print(\"\\n\\n &gt;&gt;&gt; test_define_agent_entity_create_instance\\n\")\n        agent_entity = OwlAgentEntity(agent_id=\"mistral_large\",\n                                      modelName=\"mistral-large-latest\",\n                                      modelClassName=\"langchain_mistralai.chat_models.ChatMistralAI\",\n                                      temperature=0,\n                                      )\n        assert agent_entity\n        mgr = get_agent_manager()\n        agent_executor = mgr.build_agent_from_entity(agent_entity)\n</code></pre> <p>Running this test with <code>pytest -s tests/ut/test_mistral_agent.py</code> may fail as the <code>langchain-mistralai</code> is missing in the <code>src/requirements.txt</code>. You can fix that by adding it explicitly and doing <code>pip install langchain-mistral</code>.  Once done the test should be able to create an agent instance.</p>"},{"location":"tutorials/new_mistral_agent/#adding-a-system-prompt","title":"Adding a system prompt","text":"<p>A system prompt is a text instruction to the LLM that says what is required so it can better answer the user's query. Taking the example from Mistral site, you will define a RAG system prompt which takes into account existing context.</p> <ul> <li>Add a yaml definition within the <code>tests/ut/config/prompts.yaml</code> file:</li> </ul> tests/ut/config/prompts.yaml<pre><code>mistral_rag_prompt:\n    name: Mistral Bank Query Classification\n    prompt_id: mistral_rag_prompt\n    locales:\n    - locale: en\n    text: |\n        \"Answer the following question based only on the provided context:\n\n        &lt;context&gt;\n        {context}\n        &lt;/context&gt;\n\n        Question: {input}\"\n</code></pre> <p>For unit testing, the configuration of the Owl framework is defined in <code>./tests/ut/config/config.yaml</code> and set up in each test class using the environment variable <code>CONFIG_FILE</code>.</p> <p>Add a unit test to validate that the prompt is correctly loaded:</p> tests/ut/test_mistral_agent.py<pre><code>def test_valide_prompt_is_loaded(self):\n    prompt_mgr = get_prompt_manager()\n    mistral_prompt = prompt_mgr.get_prompt(\"mistral_rag_prompt\")\n    assert mistral_prompt    # this is a string\n    print(pe)\n</code></pre> <p>Add the prompt reference to the OwlAgentEntity</p> add prompt to test_mistral_agent.py<pre><code>agent_entity = OwlAgentEntity(agent_id=\"mistral_large\",\n                                name=\"Mistral based agent\",\n                                class_name=\"athena.llm.agents.mistral_agent.MistralAgent\",\n                                modelName=\"mistral-large-latest\",\n                                prompt_ref=\"mistral_rag_prompt\",   \n)\n</code></pre> <p>For the snippet above:  The reference needs to match the <code>prompt_id</code>.</p> <p>Rerunning the unit test should succeed, but we are not yet calling the LLM. The next step is to create a LangChain chain and then do a simple invocation.</p>"},{"location":"tutorials/new_mistral_agent/#adding-a-chain","title":"Adding A Chain","text":"<p>To add a chain to the agent, add the following code:</p> Update constructor in mistral_agent.py<pre><code>    def __init__(self,agentEntity: OwlAgentEntity, prompt: BasePromptTemplate, tool_instances: Optional[list[Any]]):\n        self.prompt = prompt\n        self.model=self._instantiate_model(agentEntity.modelName, agentEntity.modelClassName, agentEntity.temperature)\n        self.llm = self.prompt | self.model | StrOutputParser()   # (1)\n\n\n    def get_runnable(self):\n        return  self.llm  # (1)\n</code></pre> <p>The lines marked <code>(1)</code> are newly added.</p> <p>The prompt has defined a new variable called <code>context</code>. The OwlFramework defines the following prompt template from the system prompt text:</p> prompt builder function in prompt_mgr.py<pre><code> ChatPromptTemplate.from_messages([\n            (\"system\", text),\n            MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n            (\"human\", \"{input}\"),\n            MessagesPlaceholder(variable_name=\"agent_scratchpad\", optional=True),\n        ])\n</code></pre> <p>The <code>chat_history</code> and <code>agent_scratchpad</code> variables are optional, but not the <code>input</code>. So you need to add some parameters to the test:</p> add invocation to test_mistral_agent.py<pre><code>    assert agent_executor\n    rep = agent_executor.invoke({\"input\" : \"What is langgraph?\", \"context\": \"\"})\n    print(rep)\n</code></pre> <p>The invocation to Mistral should work but respond something like the following answer: \"I'm sorry for the confusion, but the context provided is empty, and I don't have any information about \"langgraph\" from the given context.\"</p> <p>At this stage we have a running (albeit not particularly useful) agent, but one that can be a component for building a useful assistant that can call Mistral.</p>"},{"location":"tutorials/new_mistral_agent/#define-a-new-assistant","title":"Define a New Assistant","text":"<p>The OwlAgent Framework has some predefined agents. In the new assistant you will use the LangGraph flow for two nodes, the call to LLM and the tool node.</p> <p>The first operation is to add the new agent in <code>src/athena/config/agents.yaml</code></p> src/athena/config/agents.yaml<pre><code>mistral_large:\n  agent_id: mistral_large\n  class_name: athena.llm.agents.mistral_agent.MistralAgent\n  description: A Mistral large agent for RAG question and answer\n  modelClassName: langchain_mistralai.chat_models.ChatMistralAI\n  modelName: mistral-large-latest\n  name: Mistral based agent\n  prompt_ref: mistral_rag_prompt\n  temperature: 0\n  tools: []\n  top_k: 1\n</code></pre> <p>Then create an assistant that uses this agent:</p> src/athena/config/assistants.yaml<pre><code>mistral_tool_assistant:\n  assistant_id: mistral_tool_assistant\n  class_name: athena.llm.assistants.BaseToolGraphAssistant.BaseToolGraphAssistant\n  description: A default assistant that uses Mistral\n  name: Mistral large with tool assitant\n  agents: \n    - mistral_large\n</code></pre> <p>Now let's add a test for the assistant. For that, add a new test function:</p> add test at the API to trigger assistant execution to test_mistral_agent.py<pre><code>from athena.routers.dto_models import ConversationControl\nfrom athena.main import app\nfrom athena.app_settings import  get_config\nfrom fastapi.testclient import TestClient\n\ndef test_mistral_assistant_with_mistral_agent(self):\n        print(\"\\n\\n &gt;&gt;&gt; test_mistral_assistant_with_mistral_agent at the API level\\n\")\n        client = TestClient(app)\n        ctl = ConversationControl()\n        ctl.assistant_id=\"mistral_tool_assistant\"\n        ctl.user_id=\"test_user\"\n        ctl.thread_id=\"1\"\n        ctl.query=\"What is Athena Owl Agent?\"\n        response=client.post(get_config().api_route + \"/c/generic_chat\", json= ctl.model_dump())\n        print(f\"\\n---&gt; {response.content}\")\n</code></pre> <p>The test should fail because we have missing tools.</p>"},{"location":"tutorials/new_mistral_agent/#adding-tools-to-retrieve-documents-from-a-corpus","title":"Adding tools to retrieve documents from a corpus","text":"<p>JB to continue...</p>"}]}